Last login: Tue Sep 10 09:59:41 on ttys001
emilpaf@Emilias-MacBook-Air ~ % 
# pull Docker image for OTBTF
docker pull mdl4eo/otbtf:latest

# mount data volumes (to access data from your local machine)
docker run -it --platform=linux/amd64 -v ~/Desktop/prova_semantic_dune:/data mdl4eo/otbtf:latest /bin/bash

# change directory
cd /data/

# list files in the data folder within the directory
ls /data/

zsh: command not found: #
latest: Pulling from mdl4eo/otbtf
Digest: sha256:e6a04e727e7c4731e87ad42bacd22ab94de7efc0338439b94f96cd02ad6a9cf4
Status: Image is up to date for mdl4eo/otbtf:latest
docker.io/mdl4eo/otbtf:latest

What's next:
    View a summary of image vulnerabilities and recommendations → docker scout quickview mdl4eo/otbtf:latest
zsh: unknown sort specifier
To run a command as administrator (user "root"), use "sudo <command>".
See "man sudo_root" for details.

otbuser@81188e396eef:~$ 
# install all required python packages
pip install pyotb pystac-client planetary-computer

# verify installation of packages
python
Defaulting to user installation because normal site-packages is not writeable
Collecting pyotb
  Downloading pyotb-2.0.2-py3-none-any.whl.metadata (3.3 kB)
Collecting pystac-client
  Downloading pystac_client-0.8.3-py3-none-any.whl.metadata (5.2 kB)
Collecting planetary-computer
  Downloading planetary_computer-1.0.0-py3-none-any.whl.metadata (7.4 kB)
Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.10/dist-packages (from pyotb) (1.26.4)
Requirement already satisfied: requests>=2.28.2 in /usr/local/lib/python3.10/dist-packages (from pystac-client) (2.31.0)
Collecting pystac>=1.10.0 (from pystac[validation]>=1.10.0->pystac-client)
  Downloading pystac-1.10.1-py3-none-any.whl.metadata (6.4 kB)
Collecting python-dateutil>=2.8.2 (from pystac-client)
  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)
Collecting click>=7.1 (from planetary-computer)
  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)
Collecting pydantic>=1.7.3 (from planetary-computer)
  Downloading pydantic-2.9.1-py3-none-any.whl.metadata (146 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 147.0/147.0 kB 1.3 MB/s eta 0:00:00
Collecting pytz>=2020.5 (from planetary-computer)
  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)
Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from planetary-computer) (24.0)
Collecting python-dotenv (from planetary-computer)
  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)
Collecting annotated-types>=0.6.0 (from pydantic>=1.7.3->planetary-computer)
  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)
Collecting pydantic-core==2.23.3 (from pydantic>=1.7.3->planetary-computer)
  Downloading pydantic_core-2.23.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)
Requirement already satisfied: typing-extensions>=4.6.1 in /opt/otbtf/local/lib/python3.10/dist-packages (from pydantic>=1.7.3->planetary-computer) (4.11.0)
Collecting jsonschema~=4.18 (from pystac[validation]>=1.10.0->pystac-client)
  Downloading jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)
Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pystac-client) (1.16.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28.2->pystac-client) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28.2->pystac-client) (3.7)
Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28.2->pystac-client) (2.2.1)
Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28.2->pystac-client) (2024.2.2)
Collecting attrs>=22.2.0 (from jsonschema~=4.18->pystac[validation]>=1.10.0->pystac-client)
  Downloading attrs-24.2.0-py3-none-any.whl.metadata (11 kB)
Collecting jsonschema-specifications>=2023.03.6 (from jsonschema~=4.18->pystac[validation]>=1.10.0->pystac-client)
  Downloading jsonschema_specifications-2023.12.1-py3-none-any.whl.metadata (3.0 kB)
Collecting referencing>=0.28.4 (from jsonschema~=4.18->pystac[validation]>=1.10.0->pystac-client)
  Downloading referencing-0.35.1-py3-none-any.whl.metadata (2.8 kB)
Collecting rpds-py>=0.7.1 (from jsonschema~=4.18->pystac[validation]>=1.10.0->pystac-client)
  Downloading rpds_py-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)
Downloading pyotb-2.0.2-py3-none-any.whl (40 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.8/40.8 kB 1.2 MB/s eta 0:00:00
Downloading pystac_client-0.8.3-py3-none-any.whl (33 kB)
Downloading planetary_computer-1.0.0-py3-none-any.whl (14 kB)
Downloading click-8.1.7-py3-none-any.whl (97 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97.9/97.9 kB 3.3 MB/s eta 0:00:00
Downloading pydantic-2.9.1-py3-none-any.whl (434 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 434.4/434.4 kB 4.9 MB/s eta 0:00:00
Downloading pydantic_core-2.23.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 5.3 MB/s eta 0:00:00
Downloading pystac-1.10.1-py3-none-any.whl (182 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 182.9/182.9 kB 5.2 MB/s eta 0:00:00
Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 229.9/229.9 kB 7.8 MB/s eta 0:00:00
Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 505.5/505.5 kB 433.8 kB/s eta 0:00:00
Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)
Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)
Downloading jsonschema-4.23.0-py3-none-any.whl (88 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.5/88.5 kB 4.1 MB/s eta 0:00:00
Downloading attrs-24.2.0-py3-none-any.whl (63 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 63.0/63.0 kB 5.2 MB/s eta 0:00:00
Downloading jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)
Downloading referencing-0.35.1-py3-none-any.whl (26 kB)
Downloading rpds_py-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (354 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 354.8/354.8 kB 158.6 kB/s eta 0:00:00
Installing collected packages: pytz, rpds-py, python-dotenv, python-dateutil, pyotb, pydantic-core, click, attrs, annotated-types, referencing, pystac, pydantic, jsonschema-specifications, jsonschema, pystac-client, planetary-computer
Successfully installed annotated-types-0.7.0 attrs-24.2.0 click-8.1.7 jsonschema-4.23.0 jsonschema-specifications-2023.12.1 planetary-computer-1.0.0 pydantic-2.9.1 pydantic-core-2.23.3 pyotb-2.0.2 pystac-1.10.1 pystac-client-0.8.3 python-dateutil-2.9.0.post0 python-dotenv-1.0.1 pytz-2024.1 referencing-0.35.1 rpds-py-0.20.0

[notice] A new release of pip is available: 24.0 -> 24.2
[notice] To update, run: python3 -m pip install --upgrade pip
Python 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> 
>>> import pyotb
2024-09-10 08:12:38 (INFO) [pyotb] Successfully loaded 117 OTB applications
>>> import pystac_client
>>> import planetary_computer
>>> 
>>> import argparse
>>> import otbtf
2024-09-10 08:12:38.392630: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE3 SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.
>>> import tensorflow as tf
>>> 
>>> 
>>> 
>>> # define some constants
>>> class_nb = 3             # number of classes
>>> inp_key_ge = "input_ge"    # model input ge
>>> tgt_key = "estimated"    # model target
>>> 
>>> # helper to create otbtf dataset from lists of patches
>>> def create_otbtf_dataset(ge, labels):
...     return otbtf.DatasetFromPatchesImages(
...         filenames_dict={
...             "ge": ge,
...             "labels": labels
...         }
...     )
... 
>>> # dataset preprocessing function
>>> def dataset_preprocessing_fn(sample):
...     return {
...         inp_key_ge: sample["ge"],
...         tgt_key: otbtf.ops.one_hot(labels=sample["labels"], nb_classes=class_nb)
...     }
... 
>>> # TensorFlow dataset creation from lists of patches
>>> def create_dataset(ge, labels, batch_size=8):
...     otbtf_dataset = create_otbtf_dataset(ge, labels)
...     return otbtf_dataset.get_tf_dataset(
...         batch_size=batch_size,
...         preprocessing_fn=dataset_preprocessing_fn,
...         targets_keys=[tgt_key]
...     )
... 
>>> # define convolution operator
>>> def conv(inp, depth, name, strides=2):
...     conv_op = tf.keras.layers.Conv2D(
...         filters=depth,
...         kernel_size=3,
...         strides=strides,
...         activation="relu",
...         padding="same",
...         name=name
...     )
...     return conv_op(inp)
... 
>>> # define transposed convolution operator
>>> def tconv(inp, depth, name, activation="relu"):
...     tconv_op = tf.keras.layers.Conv2DTranspose(
...         filters=depth,
...         kernel_size=3,
...         strides=2,
...         activation=activation,
...         padding="same",
...         name=name
...     )
...     return tconv_op(inp)
... 
>>> # build the model
>>> import otbtf
>>> class FCNNModel(otbtf.ModelBase):
...     
...     def normalize_inputs(self, inputs):
...         return {
...             inp_key_ge: tf.cast(inputs[inp_key_ge], tf.float32) * 0.01,
...         }
...     
...     def get_outputs(self, normalized_inputs):
...         norm_inp_ge = normalized_inputs[inp_key_ge]
...                 
...         cv1 = conv(norm_inp_ge, 16, "conv1")
...         cv2 = conv(cv1, 32, "conv2")
...         cv3 = conv(cv2, 64, "conv3")
...         cv4 = conv(cv3, 64, "conv4")
...         cv1t = tconv(cv4, 64, "conv1t") + cv3
...         cv2t = tconv(cv1t, 32, "conv2t") + cv2
...         cv3t = tconv(cv2t, 16, "conv3t") + cv1
...         cv4t = tconv(cv3t, class_nb, "softmax_layer", "softmax")
...         
...         argmax_op = otbtf.layers.Argmax(name="argmax_layer")
...         
...         return {tgt_key: cv4t, "estimated_labels": argmax_op(cv4t)}
... 
>>> 
>>> # custom metric for F1-Score (code from: https://stackoverflow.com/questions/64474463/custom-f1-score-metric-in-tensorflow)
>>> 
>>> class FScore(tf.keras.metrics.Metric):
...         
...     def __init__(self, class_id, name=None, **kwargs):
...         if not name:
...             name = f'f_score_{class_id}'
...         super().__init__(name=name, **kwargs)
...         self.f1 = self.add_weight(name='f1', initializer='zeros')
...         self.precision_fn = tf.keras.metrics.Precision(class_id=class_id)
...         self.recall_fn = tf.keras.metrics.Recall(class_id=class_id)
...     
...     def update_state(self, y_true, y_pred, sample_weight=None):
...         p = self.precision_fn(y_true, y_pred)
...         r = self.recall_fn(y_true, y_pred)
...         # since f1 is a variable, we use assign
...         self.f1.assign(2 * ((p * r) / (p + r + 1e-6)))
...     
...     def result(self):
...         return self.f1
...     
...     def reset_state(self):
...         # we also need to reset the state of the precision and recall objects
...         self.precision_fn.reset_states()
...         self.recall_fn.reset_states()
...         self.f1.assign(0)
... 
>>> 
>>> # training setup
>>> def train(params, ds_train, ds_valid, ds_test):
...     strategy = tf.distribute.MirroredStrategy()
...     with strategy.scope():
...         model = FCNNModel(dataset_element_spec=ds_train.element_spec)
...         
...         # Precision and recall for each class
...         metrics = [
...             cls(class_id=class_id)
...             for class_id in range(class_nb)
...             for cls in [tf.keras.metrics.Precision, tf.keras.metrics.Recall]
...         ]
...         
...         # F1-Score for each class
...         metrics += [
...             FScore(class_id=class_id, name=f"fscore_cls{class_id}")
...             for class_id in range(class_nb)
...         ]
...         
...         model.compile(
...             loss={tgt_key: tf.keras.losses.CategoricalCrossentropy()},
...             optimizer=tf.keras.optimizers.Adam(params.learning_rate),
...             metrics={tgt_key: metrics}
...         )
...         model.summary()
...         save_best_cb = tf.keras.callbacks.ModelCheckpoint(
...             params.model_dir,
...             mode="min",
...             save_best_only=True,
...             monitor="val_loss"
...         )
...         callbacks = [save_best_cb]
...         if params.log_dir:
...             callbacks.append(tf.keras.callbacks.TensorBoard(log_dir=params.log_dir))
...         if params.ckpt_dir:
...             ckpt_cb = tf.keras.callbacks.BackupAndRestore(backup_dir=params.ckpt_dir)
...             callbacks.append(ckpt_cb)
...         
...         # Train the model
...         model.fit(
...             ds_train,
...             epochs=params.epochs,
...             validation_data=ds_valid,
...             callbacks=callbacks
...         )
...         
...         # Final evaluation on the test dataset
...         model.load_weights(params.model_dir)
...         values = model.evaluate(ds_test, batch_size=params.batch_size)
...         for metric_name, value in zip(model.metrics_names, values):
...             print(f"{metric_name}: {100*value:.2f}")
... 
>>> 
>>> # build a simple parser to provide arguments
>>> 
>>> import sys
>>> import argparse
>>> import tensorflow as tf
>>> 
>>> # Simulate the command-line arguments 
>>> sys.argv = [ 
...     'part_3_train.py',  # Script name 
...     '--model_dir', '/data/models/model3', 
...     '--log_dir', '/data/logs/model3',  
...     '--epochs', '50',  
...     '--ckpt_dir', '/data/ckpts/model3' 
... ]
>>> 
>>> # Now parse the arguments using argparse
>>> parser = argparse.ArgumentParser(description="Train a FCNN model")
>>> parser.add_argument("--model_dir", required=True, help="model directory")
_StoreAction(option_strings=['--model_dir'], dest='model_dir', nargs=None, const=None, default=None, type=None, choices=None, required=True, help='model directory', metavar=None)
>>> parser.add_argument("--log_dir", help="log directory")
_StoreAction(option_strings=['--log_dir'], dest='log_dir', nargs=None, const=None, default=None, type=None, choices=None, required=False, help='log directory', metavar=None)
>>> parser.add_argument("--batch_size", type=int, default=4)
_StoreAction(option_strings=['--batch_size'], dest='batch_size', nargs=None, const=None, default=4, type=<class 'int'>, choices=None, required=False, help=None, metavar=None)
>>> parser.add_argument("--learning_rate", type=float, default=0.0002)
_StoreAction(option_strings=['--learning_rate'], dest='learning_rate', nargs=None, const=None, default=0.0002, type=<class 'float'>, choices=None, required=False, help=None, metavar=None)
>>> parser.add_argument("--epochs", type=int, default=100)
_StoreAction(option_strings=['--epochs'], dest='epochs', nargs=None, const=None, default=100, type=<class 'int'>, choices=None, required=False, help=None, metavar=None)
>>> parser.add_argument("--ckpt_dir", help="Directory for checkpoints")
_StoreAction(option_strings=['--ckpt_dir'], dest='ckpt_dir', nargs=None, const=None, default=None, type=None, choices=None, required=False, help='Directory for checkpoints', metavar=None)
>>> params = parser.parse_args()
>>> 
>>> # Print to verify 
>>> print("Model directory:", params.model_dir)
Model directory: /data/models/model3
>>> 
>>> tf.get_logger().setLevel('ERROR')
>>> 
>>> 
>>> # dataset intantiation
>>> ds_train = create_dataset(
...     ["/data/train_ge_patches.tif"],
...     ["/data/train_labels_patches.tif"],
... )
/opt/otbtf/lib/python3/dist-packages/osgeo/gdal.py:315: FutureWarning: Neither gdal.UseExceptions() nor gdal.DontUseExceptions() has been explicitly called. In GDAL 4.0, exceptions will be enabled by default.
  warnings.warn(
2024-09-10 08:12:56.794578: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE3 SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.
WARNING: AutoGraph could not transform <function dataset_preprocessing_fn at 0x7fffc2c99630> and will run it as-is.
Cause: Unable to locate the source code of <function dataset_preprocessing_fn at 0x7fffc2c99630>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
>>> ds_train = ds_train.shuffle(buffer_size=100)
>>> 
>>> ds_valid = create_dataset(
...     ["/data/valid_ge_patches.tif"],
...     ["/data/valid_labels_patches.tif"],
... )
>>> 
>>> ds_test = create_dataset(
...     ["/data/test_ge_patches.tif"],
...     ["/data/test_labels_patches.tif"],
... )
>>> 
>>> 
>>> 
>>> # train the model
>>> train(params, ds_train, ds_valid, ds_test)
Model: "FCNNModel"
______________________________________________________________________________________________________________________________________________________
 Layer (type)                                Output Shape                                 Param #        Connected to                                 
======================================================================================================================================================
 input_ge (InputLayer)                       [(None, None, None, 3)]                      0              []                                           
                                                                                                                                                      
 tf.cast (TFOpLambda)                        (None, None, None, 3)                        0              ['input_ge[0][0]']                           
                                                                                                                                                      
 tf.math.multiply (TFOpLambda)               (None, None, None, 3)                        0              ['tf.cast[0][0]']                            
                                                                                                                                                      
 conv1 (Conv2D)                              (None, None, None, 16)                       448            ['tf.math.multiply[0][0]']                   
                                                                                                                                                      
 conv2 (Conv2D)                              (None, None, None, 32)                       4640           ['conv1[0][0]']                              
                                                                                                                                                      
 conv3 (Conv2D)                              (None, None, None, 64)                       18496          ['conv2[0][0]']                              
                                                                                                                                                      
 conv4 (Conv2D)                              (None, None, None, 64)                       36928          ['conv3[0][0]']                              
                                                                                                                                                      
 conv1t (Conv2DTranspose)                    (None, None, None, 64)                       36928          ['conv4[0][0]']                              
                                                                                                                                                      
 tf.__operators__.add (TFOpLambda)           (None, None, None, 64)                       0              ['conv1t[0][0]',                             
                                                                                                          'conv3[0][0]']                              
                                                                                                                                                      
 conv2t (Conv2DTranspose)                    (None, None, None, 32)                       18464          ['tf.__operators__.add[0][0]']               
                                                                                                                                                      
 tf.__operators__.add_1 (TFOpLambda)         (None, None, None, 32)                       0              ['conv2t[0][0]',                             
                                                                                                          'conv2[0][0]']                              
                                                                                                                                                      
 conv3t (Conv2DTranspose)                    (None, None, None, 16)                       4624           ['tf.__operators__.add_1[0][0]']             
                                                                                                                                                      
 tf.__operators__.add_2 (TFOpLambda)         (None, None, None, 16)                       0              ['conv3t[0][0]',                             
                                                                                                          'conv1[0][0]']                              
                                                                                                                                                      
 softmax_layer (Conv2DTranspose)             (None, None, None, 3)                        435            ['tf.__operators__.add_2[0][0]']             
                                                                                                                                                      
 argmax_layer (Argmax)                       (None, None, None, 1)                        0              ['softmax_layer[0][0]']                      
                                                                                                                                                      
 tf.__operators__.getitem_4 (SlicingOpLambd  (None, None, None, 3)                        0              ['softmax_layer[0][0]']                      
 a)                                                                                                                                                   
                                                                                                                                                      
 tf.__operators__.getitem (SlicingOpLambda)  (None, None, None, 3)                        0              ['softmax_layer[0][0]']                      
                                                                                                                                                      
 tf.__operators__.getitem_1 (SlicingOpLambd  (None, None, None, 3)                        0              ['softmax_layer[0][0]']                      
 a)                                                                                                                                                   
                                                                                                                                                      
 tf.__operators__.getitem_2 (SlicingOpLambd  (None, None, None, 3)                        0              ['softmax_layer[0][0]']                      
 a)                                                                                                                                                   
                                                                                                                                                      
 tf.__operators__.getitem_3 (SlicingOpLambd  (None, None, None, 3)                        0              ['softmax_layer[0][0]']                      
 a)                                                                                                                                                   
                                                                                                                                                      
 tf.__operators__.getitem_9 (SlicingOpLambd  (None, None, None, 1)                        0              ['argmax_layer[0][0]']                       
 a)                                                                                                                                                   
                                                                                                                                                      
 tf.__operators__.getitem_5 (SlicingOpLambd  (None, None, None, 1)                        0              ['argmax_layer[0][0]']                       
 a)                                                                                                                                                   
                                                                                                                                                      
 tf.__operators__.getitem_6 (SlicingOpLambd  (None, None, None, 1)                        0              ['argmax_layer[0][0]']                       
 a)                                                                                                                                                   
                                                                                                                                                      
 tf.__operators__.getitem_7 (SlicingOpLambd  (None, None, None, 1)                        0              ['argmax_layer[0][0]']                       
 a)                                                                                                                                                   
                                                                                                                                                      
 tf.__operators__.getitem_8 (SlicingOpLambd  (None, None, None, 1)                        0              ['argmax_layer[0][0]']                       
 a)                                                                                                                                                   
                                                                                                                                                      
 softmax_layer_crop128 (Activation)          (None, None, None, 3)                        0              ['tf.__operators__.getitem_4[0][0]']         
                                                                                                                                                      
 softmax_layer_crop16 (Activation)           (None, None, None, 3)                        0              ['tf.__operators__.getitem[0][0]']           
                                                                                                                                                      
 softmax_layer_crop32 (Activation)           (None, None, None, 3)                        0              ['tf.__operators__.getitem_1[0][0]']         
                                                                                                                                                      
 softmax_layer_crop64 (Activation)           (None, None, None, 3)                        0              ['tf.__operators__.getitem_2[0][0]']         
                                                                                                                                                      
 softmax_layer_crop96 (Activation)           (None, None, None, 3)                        0              ['tf.__operators__.getitem_3[0][0]']         
                                                                                                                                                      
 argmax_layer_crop128 (Activation)           (None, None, None, 1)                        0              ['tf.__operators__.getitem_9[0][0]']         
                                                                                                                                                      
 argmax_layer_crop16 (Activation)            (None, None, None, 1)                        0              ['tf.__operators__.getitem_5[0][0]']         
                                                                                                                                                      
 argmax_layer_crop32 (Activation)            (None, None, None, 1)                        0              ['tf.__operators__.getitem_6[0][0]']         
                                                                                                                                                      
 argmax_layer_crop64 (Activation)            (None, None, None, 1)                        0              ['tf.__operators__.getitem_7[0][0]']         
                                                                                                                                                      
 argmax_layer_crop96 (Activation)            (None, None, None, 1)                        0              ['tf.__operators__.getitem_8[0][0]']         
                                                                                                                                                      
======================================================================================================================================================
Total params: 120963 (472.51 KB)
Trainable params: 120963 (472.51 KB)
Non-trainable params: 0 (0.00 Byte)
______________________________________________________________________________________________________________________________________________________
2024-09-10 08:13:00.424368: W tensorflow/core/framework/dataset.cc:959] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.
2024-09-10 08:13:00.429347: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:118] Failed to find bogomips or clock in /proc/cpuinfo; cannot determine CPU frequency
Epoch 1/50
WARNING: AutoGraph could not transform <bound method FScore.update_state of <__main__.FScore object at 0x7fffb81cdbd0>> and will run it as-is.
Cause: Unable to locate the source code of <bound method FScore.update_state of <__main__.FScore object at 0x7fffb81cdbd0>>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <bound method FScore.result of <__main__.FScore object at 0x7fffb81cdbd0>> and will run it as-is.
Cause: Unable to locate the source code of <bound method FScore.result of <__main__.FScore object at 0x7fffb81cdbd0>>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
60/60 [==============================] - 4s 39ms/step - loss: 0.3662 - softmax_layer_loss: 0.3662 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.2934 - softmax_layer_recall_1: 0.0704 - softmax_layer_precision_2: 0.1375 - softmax_layer_recall_2: 0.0464 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1136 - softmax_layer_fscore_cls2: 0.0694 - val_loss: 0.3273 - val_softmax_layer_loss: 0.3273 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1734 - val_softmax_layer_recall_1: 0.2354 - val_softmax_layer_precision_2: 0.1195 - val_softmax_layer_recall_2: 0.2076 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.1997 - val_softmax_layer_fscore_cls2: 0.1517
Epoch 2/50
60/60 [==============================] - 1s 13ms/step - loss: 1.8243 - softmax_layer_loss: 1.8243 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1185 - softmax_layer_recall_1: 0.3722 - softmax_layer_precision_2: 0.2146 - softmax_layer_recall_2: 0.2908 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1798 - softmax_layer_fscore_cls2: 0.2470 - val_loss: 11.3279 - val_softmax_layer_loss: 11.3279 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1513 - val_softmax_layer_recall_1: 0.4623 - val_softmax_layer_precision_2: 0.1707 - val_softmax_layer_recall_2: 0.3156 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.2280 - val_softmax_layer_fscore_cls2: 0.2216
Epoch 3/50
60/60 [==============================] - 1s 13ms/step - loss: 235.0348 - softmax_layer_loss: 235.0348 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1080 - softmax_layer_recall_1: 0.4454 - softmax_layer_precision_2: 0.2249 - softmax_layer_recall_2: 0.4120 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1739 - softmax_layer_fscore_cls2: 0.2910 - val_loss: 1134.6652 - val_softmax_layer_loss: 1134.6652 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1377 - val_softmax_layer_recall_1: 0.4782 - val_softmax_layer_precision_2: 0.1698 - val_softmax_layer_recall_2: 0.4180 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.2138 - val_softmax_layer_fscore_cls2: 0.2415
Epoch 4/50
60/60 [==============================] - 1s 13ms/step - loss: 9141.8330 - softmax_layer_loss: 9141.8330 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1097 - softmax_layer_recall_1: 0.4841 - softmax_layer_precision_2: 0.2296 - softmax_layer_recall_2: 0.4444 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1788 - softmax_layer_fscore_cls2: 0.3027 - val_loss: 26768.4785 - val_softmax_layer_loss: 26768.4785 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1370 - val_softmax_layer_recall_1: 0.5056 - val_softmax_layer_precision_2: 0.1694 - val_softmax_layer_recall_2: 0.4412 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.2156 - val_softmax_layer_fscore_cls2: 0.2448
Epoch 5/50
60/60 [==============================] - 1s 13ms/step - loss: 101453.5312 - softmax_layer_loss: 101453.5312 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1098 - softmax_layer_recall_1: 0.4999 - softmax_layer_precision_2: 0.2286 - softmax_layer_recall_2: 0.4352 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1801 - softmax_layer_fscore_cls2: 0.2998 - val_loss: 246339.0781 - val_softmax_layer_loss: 246339.0781 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1179 - val_softmax_layer_recall_1: 0.4769 - val_softmax_layer_precision_2: 0.1403 - val_softmax_layer_recall_2: 0.4672 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.1891 - val_softmax_layer_fscore_cls2: 0.2158
Epoch 6/50
60/60 [==============================] - 1s 13ms/step - loss: 761248.4375 - softmax_layer_loss: 761248.4375 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1113 - softmax_layer_recall_1: 0.4926 - softmax_layer_precision_2: 0.2267 - softmax_layer_recall_2: 0.4566 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1816 - softmax_layer_fscore_cls2: 0.3030 - val_loss: 1569395.8750 - val_softmax_layer_loss: 1569395.8750 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1181 - val_softmax_layer_recall_1: 0.4136 - val_softmax_layer_precision_2: 0.1604 - val_softmax_layer_recall_2: 0.5444 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.1837 - val_softmax_layer_fscore_cls2: 0.2478
Epoch 7/50
60/60 [==============================] - 1s 13ms/step - loss: 3814464.7500 - softmax_layer_loss: 3814464.7500 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1038 - softmax_layer_recall_1: 0.4713 - softmax_layer_precision_2: 0.2328 - softmax_layer_recall_2: 0.4759 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1702 - softmax_layer_fscore_cls2: 0.3127 - val_loss: 7475972.0000 - val_softmax_layer_loss: 7475972.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1499 - val_softmax_layer_recall_1: 0.5316 - val_softmax_layer_precision_2: 0.1491 - val_softmax_layer_recall_2: 0.4337 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.2338 - val_softmax_layer_fscore_cls2: 0.2219
Epoch 8/50
60/60 [==============================] - 1s 13ms/step - loss: 12904528.0000 - softmax_layer_loss: 12904528.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1076 - softmax_layer_recall_1: 0.4881 - softmax_layer_precision_2: 0.2323 - softmax_layer_recall_2: 0.4737 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1764 - softmax_layer_fscore_cls2: 0.3117 - val_loss: 23817202.0000 - val_softmax_layer_loss: 23817202.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1371 - val_softmax_layer_recall_1: 0.5465 - val_softmax_layer_precision_2: 0.1488 - val_softmax_layer_recall_2: 0.4155 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.2193 - val_softmax_layer_fscore_cls2: 0.2192
Epoch 9/50
60/60 [==============================] - 1s 13ms/step - loss: 43212752.0000 - softmax_layer_loss: 43212752.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1084 - softmax_layer_recall_1: 0.4944 - softmax_layer_precision_2: 0.2314 - softmax_layer_recall_2: 0.4626 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1778 - softmax_layer_fscore_cls2: 0.3085 - val_loss: 51201372.0000 - val_softmax_layer_loss: 51201372.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1014 - val_softmax_layer_recall_1: 0.5951 - val_softmax_layer_precision_2: 0.1353 - val_softmax_layer_recall_2: 0.3923 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.1732 - val_softmax_layer_fscore_cls2: 0.2012
Epoch 10/50
60/60 [==============================] - 1s 13ms/step - loss: 108404368.0000 - softmax_layer_loss: 108404368.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1065 - softmax_layer_recall_1: 0.4961 - softmax_layer_precision_2: 0.2295 - softmax_layer_recall_2: 0.4617 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1754 - softmax_layer_fscore_cls2: 0.3066 - val_loss: 146726640.0000 - val_softmax_layer_loss: 146726640.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1326 - val_softmax_layer_recall_1: 0.4224 - val_softmax_layer_precision_2: 0.1255 - val_softmax_layer_recall_2: 0.5449 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.2018 - val_softmax_layer_fscore_cls2: 0.2040
Epoch 11/50
60/60 [==============================] - 1s 13ms/step - loss: 240951712.0000 - softmax_layer_loss: 240951712.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1045 - softmax_layer_recall_1: 0.4763 - softmax_layer_precision_2: 0.2275 - softmax_layer_recall_2: 0.4669 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1714 - softmax_layer_fscore_cls2: 0.3059 - val_loss: 356719104.0000 - val_softmax_layer_loss: 356719104.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1342 - val_softmax_layer_recall_1: 0.5267 - val_softmax_layer_precision_2: 0.1688 - val_softmax_layer_recall_2: 0.3918 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.2138 - val_softmax_layer_fscore_cls2: 0.2360
Epoch 12/50
60/60 [==============================] - 1s 13ms/step - loss: 497079104.0000 - softmax_layer_loss: 497079104.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1071 - softmax_layer_recall_1: 0.4804 - softmax_layer_precision_2: 0.2266 - softmax_layer_recall_2: 0.4652 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1752 - softmax_layer_fscore_cls2: 0.3048 - val_loss: 589170688.0000 - val_softmax_layer_loss: 589170688.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1213 - val_softmax_layer_recall_1: 0.4792 - val_softmax_layer_precision_2: 0.1360 - val_softmax_layer_recall_2: 0.4833 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.1936 - val_softmax_layer_fscore_cls2: 0.2122
Epoch 13/50
60/60 [==============================] - 1s 13ms/step - loss: 969049536.0000 - softmax_layer_loss: 969049536.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1120 - softmax_layer_recall_1: 0.4880 - softmax_layer_precision_2: 0.2282 - softmax_layer_recall_2: 0.4689 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1822 - softmax_layer_fscore_cls2: 0.3070 - val_loss: 1244146688.0000 - val_softmax_layer_loss: 1244146688.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1315 - val_softmax_layer_recall_1: 0.4879 - val_softmax_layer_precision_2: 0.1672 - val_softmax_layer_recall_2: 0.4782 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.2071 - val_softmax_layer_fscore_cls2: 0.2478
Epoch 14/50
60/60 [==============================] - 1s 13ms/step - loss: 1700640384.0000 - softmax_layer_loss: 1700640384.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1044 - softmax_layer_recall_1: 0.4650 - softmax_layer_precision_2: 0.2228 - softmax_layer_recall_2: 0.4579 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1706 - softmax_layer_fscore_cls2: 0.2998 - val_loss: 1834259456.0000 - val_softmax_layer_loss: 1834259456.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1086 - val_softmax_layer_recall_1: 0.5321 - val_softmax_layer_precision_2: 0.1565 - val_softmax_layer_recall_2: 0.4248 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.1803 - val_softmax_layer_fscore_cls2: 0.2288
Epoch 15/50
60/60 [==============================] - 1s 13ms/step - loss: 2855328512.0000 - softmax_layer_loss: 2855328512.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1107 - softmax_layer_recall_1: 0.4874 - softmax_layer_precision_2: 0.2302 - softmax_layer_recall_2: 0.4623 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1804 - softmax_layer_fscore_cls2: 0.3073 - val_loss: 3664553216.0000 - val_softmax_layer_loss: 3664553216.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1467 - val_softmax_layer_recall_1: 0.5350 - val_softmax_layer_precision_2: 0.1496 - val_softmax_layer_recall_2: 0.4176 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.2302 - val_softmax_layer_fscore_cls2: 0.2203
Epoch 16/50
60/60 [==============================] - 1s 13ms/step - loss: 4351783424.0000 - softmax_layer_loss: 4351783424.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1068 - softmax_layer_recall_1: 0.4768 - softmax_layer_precision_2: 0.2291 - softmax_layer_recall_2: 0.4501 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1745 - softmax_layer_fscore_cls2: 0.3036 - val_loss: 5740410368.0000 - val_softmax_layer_loss: 5740410368.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1490 - val_softmax_layer_recall_1: 0.5219 - val_softmax_layer_precision_2: 0.1496 - val_softmax_layer_recall_2: 0.4273 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.2319 - val_softmax_layer_fscore_cls2: 0.2216
Epoch 17/50
60/60 [==============================] - 1s 13ms/step - loss: 6850098688.0000 - softmax_layer_loss: 6850098688.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1096 - softmax_layer_recall_1: 0.4850 - softmax_layer_precision_2: 0.2291 - softmax_layer_recall_2: 0.4578 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1789 - softmax_layer_fscore_cls2: 0.3054 - val_loss: 8371298816.0000 - val_softmax_layer_loss: 8371298816.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1360 - val_softmax_layer_recall_1: 0.4270 - val_softmax_layer_precision_2: 0.1687 - val_softmax_layer_recall_2: 0.4699 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.2063 - val_softmax_layer_fscore_cls2: 0.2483
Epoch 18/50
60/60 [==============================] - 1s 13ms/step - loss: 10302433280.0000 - softmax_layer_loss: 10302433280.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1104 - softmax_layer_recall_1: 0.4854 - softmax_layer_precision_2: 0.2286 - softmax_layer_recall_2: 0.4496 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1799 - softmax_layer_fscore_cls2: 0.3031 - val_loss: 9494159360.0000 - val_softmax_layer_loss: 9494159360.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.0955 - val_softmax_layer_recall_1: 0.4191 - val_softmax_layer_precision_2: 0.1550 - val_softmax_layer_recall_2: 0.5268 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.1555 - val_softmax_layer_fscore_cls2: 0.2396
Epoch 19/50
60/60 [==============================] - 1s 13ms/step - loss: 14334636032.0000 - softmax_layer_loss: 14334636032.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1078 - softmax_layer_recall_1: 0.4738 - softmax_layer_precision_2: 0.2351 - softmax_layer_recall_2: 0.4581 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1756 - softmax_layer_fscore_cls2: 0.3107 - val_loss: 19247597568.0000 - val_softmax_layer_loss: 19247597568.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1490 - val_softmax_layer_recall_1: 0.4796 - val_softmax_layer_precision_2: 0.1705 - val_softmax_layer_recall_2: 0.4755 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.2274 - val_softmax_layer_fscore_cls2: 0.2510
Epoch 20/50
60/60 [==============================] - 1s 14ms/step - loss: 19924295680.0000 - softmax_layer_loss: 19924295680.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1090 - softmax_layer_recall_1: 0.4946 - softmax_layer_precision_2: 0.2288 - softmax_layer_recall_2: 0.4432 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1786 - softmax_layer_fscore_cls2: 0.3018 - val_loss: 26405380096.0000 - val_softmax_layer_loss: 26405380096.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1489 - val_softmax_layer_recall_1: 0.3798 - val_softmax_layer_precision_2: 0.1705 - val_softmax_layer_recall_2: 0.5365 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.2140 - val_softmax_layer_fscore_cls2: 0.2588
Epoch 21/50
60/60 [==============================] - 1s 13ms/step - loss: 27259006976.0000 - softmax_layer_loss: 27259006976.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1084 - softmax_layer_recall_1: 0.4741 - softmax_layer_precision_2: 0.2280 - softmax_layer_recall_2: 0.4523 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1764 - softmax_layer_fscore_cls2: 0.3032 - val_loss: 36812369920.0000 - val_softmax_layer_loss: 36812369920.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1488 - val_softmax_layer_recall_1: 0.3730 - val_softmax_layer_precision_2: 0.1704 - val_softmax_layer_recall_2: 0.5777 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.2128 - val_softmax_layer_fscore_cls2: 0.2631
Epoch 22/50
60/60 [==============================] - 1s 13ms/step - loss: 37188067328.0000 - softmax_layer_loss: 37188067328.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1069 - softmax_layer_recall_1: 0.4651 - softmax_layer_precision_2: 0.2348 - softmax_layer_recall_2: 0.4704 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1738 - softmax_layer_fscore_cls2: 0.3132 - val_loss: 40446623744.0000 - val_softmax_layer_loss: 40446623744.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1269 - val_softmax_layer_recall_1: 0.5146 - val_softmax_layer_precision_2: 0.1092 - val_softmax_layer_recall_2: 0.4279 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.2035 - val_softmax_layer_fscore_cls2: 0.1740
Epoch 23/50
60/60 [==============================] - 1s 13ms/step - loss: 47385108480.0000 - softmax_layer_loss: 47385108480.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1038 - softmax_layer_recall_1: 0.4669 - softmax_layer_precision_2: 0.2245 - softmax_layer_recall_2: 0.4550 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1698 - softmax_layer_fscore_cls2: 0.3007 - val_loss: 50411737088.0000 - val_softmax_layer_loss: 50411737088.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1172 - val_softmax_layer_recall_1: 0.5507 - val_softmax_layer_precision_2: 0.1399 - val_softmax_layer_recall_2: 0.4043 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.1933 - val_softmax_layer_fscore_cls2: 0.2079
Epoch 24/50
60/60 [==============================] - 1s 13ms/step - loss: 62543040512.0000 - softmax_layer_loss: 62543040512.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1053 - softmax_layer_recall_1: 0.4652 - softmax_layer_precision_2: 0.2301 - softmax_layer_recall_2: 0.4701 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1718 - softmax_layer_fscore_cls2: 0.3090 - val_loss: 77872898048.0000 - val_softmax_layer_loss: 77872898048.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1489 - val_softmax_layer_recall_1: 0.6224 - val_softmax_layer_precision_2: 0.1496 - val_softmax_layer_recall_2: 0.3158 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.2403 - val_softmax_layer_fscore_cls2: 0.2031
Epoch 25/50
60/60 [==============================] - 1s 13ms/step - loss: 80051290112.0000 - softmax_layer_loss: 80051290112.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1112 - softmax_layer_recall_1: 0.4939 - softmax_layer_precision_2: 0.2307 - softmax_layer_recall_2: 0.4565 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1816 - softmax_layer_fscore_cls2: 0.3065 - val_loss: 85151703040.0000 - val_softmax_layer_loss: 85151703040.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1100 - val_softmax_layer_recall_1: 0.3827 - val_softmax_layer_precision_2: 0.1445 - val_softmax_layer_recall_2: 0.5329 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.1709 - val_softmax_layer_fscore_cls2: 0.2273
Epoch 26/50
60/60 [==============================] - 1s 13ms/step - loss: 103899742208.0000 - softmax_layer_loss: 103899742208.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1124 - softmax_layer_recall_1: 0.4810 - softmax_layer_precision_2: 0.2343 - softmax_layer_recall_2: 0.4666 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1823 - softmax_layer_fscore_cls2: 0.3119 - val_loss: 110506024960.0000 - val_softmax_layer_loss: 110506024960.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1313 - val_softmax_layer_recall_1: 0.4647 - val_softmax_layer_precision_2: 0.1464 - val_softmax_layer_recall_2: 0.4817 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.2047 - val_softmax_layer_fscore_cls2: 0.2245
Epoch 27/50
60/60 [==============================] - 1s 13ms/step - loss: 126032592896.0000 - softmax_layer_loss: 126032592896.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1072 - softmax_layer_recall_1: 0.4684 - softmax_layer_precision_2: 0.2343 - softmax_layer_recall_2: 0.4691 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1745 - softmax_layer_fscore_cls2: 0.3125 - val_loss: 155208400896.0000 - val_softmax_layer_loss: 155208400896.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1487 - val_softmax_layer_recall_1: 0.5373 - val_softmax_layer_precision_2: 0.1490 - val_softmax_layer_recall_2: 0.4415 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.2329 - val_softmax_layer_fscore_cls2: 0.2228
Epoch 28/50
60/60 [==============================] - 1s 13ms/step - loss: 157973512192.0000 - softmax_layer_loss: 157973512192.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1071 - softmax_layer_recall_1: 0.4618 - softmax_layer_precision_2: 0.2309 - softmax_layer_recall_2: 0.4627 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1739 - softmax_layer_fscore_cls2: 0.3081 - val_loss: 149665857536.0000 - val_softmax_layer_loss: 149665857536.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1094 - val_softmax_layer_recall_1: 0.6414 - val_softmax_layer_precision_2: 0.1559 - val_softmax_layer_recall_2: 0.3165 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.1870 - val_softmax_layer_fscore_cls2: 0.2089
Epoch 29/50
60/60 [==============================] - 1s 13ms/step - loss: 189831102464.0000 - softmax_layer_loss: 189831102464.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1103 - softmax_layer_recall_1: 0.4743 - softmax_layer_precision_2: 0.2310 - softmax_layer_recall_2: 0.4690 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1789 - softmax_layer_fscore_cls2: 0.3095 - val_loss: 218701742080.0000 - val_softmax_layer_loss: 218701742080.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1487 - val_softmax_layer_recall_1: 0.5736 - val_softmax_layer_precision_2: 0.1499 - val_softmax_layer_recall_2: 0.3867 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.2361 - val_softmax_layer_fscore_cls2: 0.2160
Epoch 30/50
60/60 [==============================] - 1s 13ms/step - loss: 230472335360.0000 - softmax_layer_loss: 230472335360.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1080 - softmax_layer_recall_1: 0.4676 - softmax_layer_precision_2: 0.2177 - softmax_layer_recall_2: 0.4420 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1755 - softmax_layer_fscore_cls2: 0.2917 - val_loss: 279841931264.0000 - val_softmax_layer_loss: 279841931264.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1492 - val_softmax_layer_recall_1: 0.4341 - val_softmax_layer_precision_2: 0.1497 - val_softmax_layer_recall_2: 0.4148 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.2221 - val_softmax_layer_fscore_cls2: 0.2200
Epoch 31/50
55/60 [==========================>...] - ETA: 0s - loss: 278877208576.0000 - softmax_layer_loss: 278877208576.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1125 - softmax_layer_recall_1: 0.4641 - softmax_layer_precision_2: 0.2307 - softmax_layer_recall_2: 0.4604 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1811 - softmax_layer_fscore_cls2: 0.30732024-09-10 08:13:31.044588: W tensorflow/core/framework/dataset.cc:959] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.
60/60 [==============================] - 1s 13ms/step - loss: 271665233920.0000 - softmax_layer_loss: 271665233920.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1086 - softmax_layer_recall_1: 0.4622 - softmax_layer_precision_2: 0.2279 - softmax_layer_recall_2: 0.4632 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1759 - softmax_layer_fscore_cls2: 0.3055 - val_loss: 317295132672.0000 - val_softmax_layer_loss: 317295132672.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1360 - val_softmax_layer_recall_1: 0.3761 - val_softmax_layer_precision_2: 0.1623 - val_softmax_layer_recall_2: 0.5494 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.1998 - val_softmax_layer_fscore_cls2: 0.2506
Epoch 32/50
60/60 [==============================] - 1s 13ms/step - loss: 325286658048.0000 - softmax_layer_loss: 325286658048.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1146 - softmax_layer_recall_1: 0.4752 - softmax_layer_precision_2: 0.2218 - softmax_layer_recall_2: 0.4610 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1847 - softmax_layer_fscore_cls2: 0.2995 - val_loss: 409263570944.0000 - val_softmax_layer_loss: 409263570944.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1489 - val_softmax_layer_recall_1: 0.5682 - val_softmax_layer_precision_2: 0.1704 - val_softmax_layer_recall_2: 0.3652 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.2359 - val_softmax_layer_fscore_cls2: 0.2324
Epoch 33/50
60/60 [==============================] - 1s 13ms/step - loss: 383498682368.0000 - softmax_layer_loss: 383498682368.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1107 - softmax_layer_recall_1: 0.4702 - softmax_layer_precision_2: 0.2306 - softmax_layer_recall_2: 0.4806 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1793 - softmax_layer_fscore_cls2: 0.3116 - val_loss: 505130287104.0000 - val_softmax_layer_loss: 505130287104.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1483 - val_softmax_layer_recall_1: 0.3763 - val_softmax_layer_precision_2: 0.1701 - val_softmax_layer_recall_2: 0.4614 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.2128 - val_softmax_layer_fscore_cls2: 0.2485
Epoch 34/50
60/60 [==============================] - 1s 13ms/step - loss: 464997187584.0000 - softmax_layer_loss: 464997187584.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1090 - softmax_layer_recall_1: 0.4558 - softmax_layer_precision_2: 0.2308 - softmax_layer_recall_2: 0.4692 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1759 - softmax_layer_fscore_cls2: 0.3094 - val_loss: 474285867008.0000 - val_softmax_layer_loss: 474285867008.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1177 - val_softmax_layer_recall_1: 0.3297 - val_softmax_layer_precision_2: 0.1486 - val_softmax_layer_recall_2: 0.6193 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.1734 - val_softmax_layer_fscore_cls2: 0.2397
Epoch 35/50
60/60 [==============================] - 1s 13ms/step - loss: 529416585216.0000 - softmax_layer_loss: 529416585216.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1038 - softmax_layer_recall_1: 0.4371 - softmax_layer_precision_2: 0.2232 - softmax_layer_recall_2: 0.4677 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1678 - softmax_layer_fscore_cls2: 0.3022 - val_loss: 589564411904.0000 - val_softmax_layer_loss: 589564411904.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1352 - val_softmax_layer_recall_1: 0.4418 - val_softmax_layer_precision_2: 0.1637 - val_softmax_layer_recall_2: 0.4754 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.2070 - val_softmax_layer_fscore_cls2: 0.2436
Epoch 36/50
60/60 [==============================] - 1s 13ms/step - loss: 617506209792.0000 - softmax_layer_loss: 617506209792.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1115 - softmax_layer_recall_1: 0.4670 - softmax_layer_precision_2: 0.2297 - softmax_layer_recall_2: 0.4808 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1800 - softmax_layer_fscore_cls2: 0.3109 - val_loss: 667882029056.0000 - val_softmax_layer_loss: 667882029056.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1176 - val_softmax_layer_recall_1: 0.6394 - val_softmax_layer_precision_2: 0.1698 - val_softmax_layer_recall_2: 0.2877 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.1987 - val_softmax_layer_fscore_cls2: 0.2135
Epoch 37/50
60/60 [==============================] - 1s 13ms/step - loss: 725849145344.0000 - softmax_layer_loss: 725849145344.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1105 - softmax_layer_recall_1: 0.4644 - softmax_layer_precision_2: 0.2249 - softmax_layer_recall_2: 0.4622 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1786 - softmax_layer_fscore_cls2: 0.3026 - val_loss: 689979916288.0000 - val_softmax_layer_loss: 689979916288.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1132 - val_softmax_layer_recall_1: 0.5566 - val_softmax_layer_precision_2: 0.1436 - val_softmax_layer_recall_2: 0.3131 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.1881 - val_softmax_layer_fscore_cls2: 0.1969
Epoch 38/50
60/60 [==============================] - 1s 12ms/step - loss: 844674105344.0000 - softmax_layer_loss: 844674105344.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1075 - softmax_layer_recall_1: 0.4505 - softmax_layer_precision_2: 0.2277 - softmax_layer_recall_2: 0.4690 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1736 - softmax_layer_fscore_cls2: 0.3066 - val_loss: 1053601103872.0000 - val_softmax_layer_loss: 1053601103872.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1493 - val_softmax_layer_recall_1: 0.3345 - val_softmax_layer_precision_2: 0.1701 - val_softmax_layer_recall_2: 0.5800 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.2065 - val_softmax_layer_fscore_cls2: 0.2631
Epoch 39/50
60/60 [==============================] - 1s 12ms/step - loss: 961386643456.0000 - softmax_layer_loss: 961386643456.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1125 - softmax_layer_recall_1: 0.4594 - softmax_layer_precision_2: 0.2238 - softmax_layer_recall_2: 0.4697 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1807 - softmax_layer_fscore_cls2: 0.3032 - val_loss: 1066295296000.0000 - val_softmax_layer_loss: 1066295296000.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1364 - val_softmax_layer_recall_1: 0.6024 - val_softmax_layer_precision_2: 0.1495 - val_softmax_layer_recall_2: 0.2921 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.2225 - val_softmax_layer_fscore_cls2: 0.1978
Epoch 40/50
60/60 [==============================] - 1s 13ms/step - loss: 1097487220736.0000 - softmax_layer_loss: 1097487220736.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1061 - softmax_layer_recall_1: 0.4464 - softmax_layer_precision_2: 0.2303 - softmax_layer_recall_2: 0.4680 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1715 - softmax_layer_fscore_cls2: 0.3087 - val_loss: 1244743204864.0000 - val_softmax_layer_loss: 1244743204864.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1355 - val_softmax_layer_recall_1: 0.2704 - val_softmax_layer_precision_2: 0.1207 - val_softmax_layer_recall_2: 0.6945 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.1806 - val_softmax_layer_fscore_cls2: 0.2056
Epoch 41/50
60/60 [==============================] - 1s 13ms/step - loss: 1235115704320.0000 - softmax_layer_loss: 1235115704320.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1083 - softmax_layer_recall_1: 0.4508 - softmax_layer_precision_2: 0.2326 - softmax_layer_recall_2: 0.4779 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1747 - softmax_layer_fscore_cls2: 0.3129 - val_loss: 1293340377088.0000 - val_softmax_layer_loss: 1293340377088.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1255 - val_softmax_layer_recall_1: 0.3489 - val_softmax_layer_precision_2: 0.1311 - val_softmax_layer_recall_2: 0.4937 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.1846 - val_softmax_layer_fscore_cls2: 0.2072
Epoch 42/50
60/60 [==============================] - 1s 13ms/step - loss: 1373043818496.0000 - softmax_layer_loss: 1373043818496.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1029 - softmax_layer_recall_1: 0.4327 - softmax_layer_precision_2: 0.2239 - softmax_layer_recall_2: 0.4653 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1663 - softmax_layer_fscore_cls2: 0.3023 - val_loss: 1564061204480.0000 - val_softmax_layer_loss: 1564061204480.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1309 - val_softmax_layer_recall_1: 0.4118 - val_softmax_layer_precision_2: 0.1674 - val_softmax_layer_recall_2: 0.5110 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.1987 - val_softmax_layer_fscore_cls2: 0.2521
Epoch 43/50
60/60 [==============================] - 1s 13ms/step - loss: 1547785994240.0000 - softmax_layer_loss: 1547785994240.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1116 - softmax_layer_recall_1: 0.4676 - softmax_layer_precision_2: 0.2355 - softmax_layer_recall_2: 0.4850 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1802 - softmax_layer_fscore_cls2: 0.3171 - val_loss: 1794882797568.0000 - val_softmax_layer_loss: 1794882797568.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1485 - val_softmax_layer_recall_1: 0.4672 - val_softmax_layer_precision_2: 0.1490 - val_softmax_layer_recall_2: 0.4437 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.2254 - val_softmax_layer_fscore_cls2: 0.2230
Epoch 44/50
60/60 [==============================] - 1s 13ms/step - loss: 1742193426432.0000 - softmax_layer_loss: 1742193426432.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1152 - softmax_layer_recall_1: 0.4763 - softmax_layer_precision_2: 0.2222 - softmax_layer_recall_2: 0.4607 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1855 - softmax_layer_fscore_cls2: 0.2998 - val_loss: 2002176049152.0000 - val_softmax_layer_loss: 2002176049152.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1488 - val_softmax_layer_recall_1: 0.5674 - val_softmax_layer_precision_2: 0.1489 - val_softmax_layer_recall_2: 0.4109 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.2358 - val_softmax_layer_fscore_cls2: 0.2186
Epoch 45/50
60/60 [==============================] - 1s 13ms/step - loss: 1988258955264.0000 - softmax_layer_loss: 1988258955264.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1106 - softmax_layer_recall_1: 0.4477 - softmax_layer_precision_2: 0.2261 - softmax_layer_recall_2: 0.4734 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1774 - softmax_layer_fscore_cls2: 0.3061 - val_loss: 1873709498368.0000 - val_softmax_layer_loss: 1873709498368.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1208 - val_softmax_layer_recall_1: 0.6136 - val_softmax_layer_precision_2: 0.1356 - val_softmax_layer_recall_2: 0.2731 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.2019 - val_softmax_layer_fscore_cls2: 0.1812
Epoch 46/50
60/60 [==============================] - 1s 13ms/step - loss: 2162758516736.0000 - softmax_layer_loss: 2162758516736.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1052 - softmax_layer_recall_1: 0.4391 - softmax_layer_precision_2: 0.2304 - softmax_layer_recall_2: 0.4808 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1698 - softmax_layer_fscore_cls2: 0.3115 - val_loss: 2472535654400.0000 - val_softmax_layer_loss: 2472535654400.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1364 - val_softmax_layer_recall_1: 0.6001 - val_softmax_layer_precision_2: 0.1693 - val_softmax_layer_recall_2: 0.3461 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.2223 - val_softmax_layer_fscore_cls2: 0.2274
Epoch 47/50
60/60 [==============================] - 1s 12ms/step - loss: 2385187438592.0000 - softmax_layer_loss: 2385187438592.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1050 - softmax_layer_recall_1: 0.4376 - softmax_layer_precision_2: 0.2321 - softmax_layer_recall_2: 0.4721 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1694 - softmax_layer_fscore_cls2: 0.3112 - val_loss: 2637492912128.0000 - val_softmax_layer_loss: 2637492912128.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1309 - val_softmax_layer_recall_1: 0.2980 - val_softmax_layer_precision_2: 0.1257 - val_softmax_layer_recall_2: 0.6597 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.1819 - val_softmax_layer_fscore_cls2: 0.2111
Epoch 48/50
60/60 [==============================] - 1s 13ms/step - loss: 2716347924480.0000 - softmax_layer_loss: 2716347924480.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1107 - softmax_layer_recall_1: 0.4511 - softmax_layer_precision_2: 0.2311 - softmax_layer_recall_2: 0.4834 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1778 - softmax_layer_fscore_cls2: 0.3127 - val_loss: 3026672943104.0000 - val_softmax_layer_loss: 3026672943104.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1369 - val_softmax_layer_recall_1: 0.3545 - val_softmax_layer_precision_2: 0.1611 - val_softmax_layer_recall_2: 0.5331 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.1975 - val_softmax_layer_fscore_cls2: 0.2474
Epoch 49/50
60/60 [==============================] - 1s 13ms/step - loss: 3017349267456.0000 - softmax_layer_loss: 3017349267456.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1095 - softmax_layer_recall_1: 0.4527 - softmax_layer_precision_2: 0.2261 - softmax_layer_recall_2: 0.4658 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1763 - softmax_layer_fscore_cls2: 0.3044 - val_loss: 2625341751296.0000 - val_softmax_layer_loss: 2625341751296.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1019 - val_softmax_layer_recall_1: 0.6280 - val_softmax_layer_precision_2: 0.1402 - val_softmax_layer_recall_2: 0.3105 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.1753 - val_softmax_layer_fscore_cls2: 0.1932
Epoch 50/50
60/60 [==============================] - 1s 12ms/step - loss: 3251710984192.0000 - softmax_layer_loss: 3251710984192.0000 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1107 - softmax_layer_recall_1: 0.4529 - softmax_layer_precision_2: 0.2285 - softmax_layer_recall_2: 0.4714 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1780 - softmax_layer_fscore_cls2: 0.3078 - val_loss: 3929501597696.0000 - val_softmax_layer_loss: 3929501597696.0000 - val_softmax_layer_precision: 0.0000e+00 - val_softmax_layer_recall: 0.0000e+00 - val_softmax_layer_precision_1: 0.1490 - val_softmax_layer_recall_1: 0.5550 - val_softmax_layer_precision_2: 0.1699 - val_softmax_layer_recall_2: 0.3930 - val_softmax_layer_fscore_cls0: 0.0000e+00 - val_softmax_layer_fscore_cls1: 0.2349 - val_softmax_layer_fscore_cls2: 0.2372
2024-09-10 08:13:48.045467: W tensorflow/core/util/tensor_slice_reader.cc:98] Could not open /data/models/model3: FAILED_PRECONDITION: /data/models/model3; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?
7/7 [==============================] - 0s 8ms/step - loss: 0.3157 - softmax_layer_loss: 0.3157 - softmax_layer_precision: 0.0000e+00 - softmax_layer_recall: 0.0000e+00 - softmax_layer_precision_1: 0.1112 - softmax_layer_recall_1: 0.2344 - softmax_layer_precision_2: 0.1507 - softmax_layer_recall_2: 0.1727 - softmax_layer_fscore_cls0: 0.0000e+00 - softmax_layer_fscore_cls1: 0.1509 - softmax_layer_fscore_cls2: 0.1610
loss: 31.57
softmax_layer_loss: 31.57
softmax_layer_precision: 0.00
softmax_layer_recall: 0.00
softmax_layer_precision_1: 11.12
softmax_layer_recall_1: 23.44
softmax_layer_precision_2: 15.07
softmax_layer_recall_2: 17.27
softmax_layer_fscore_cls0: 0.00
softmax_layer_fscore_cls1: 15.09
softmax_layer_fscore_cls2: 16.10
>>> 
>>> 
>>> import sys
>>> import argparse
>>> import tensorflow as tf
>>> 
>>> # Simulate the command-line arguments 
>>> sys.argv = [ 
...     'part_3_train.py',  # Script name 
...     '--model_dir', '/data/models/model3'
... ]
>>> 
>>> import pyotb
>>> 
>>> # Generate the classification map
>>> infer = pyotb.TensorflowModelServe(
...   n_sources=1,
...   source1_il="/data/GE_aoi1.tif",
...   source1_rfieldx=128,
...   source1_rfieldy=128,
...   source1_placeholder="input_ge",
...   model_dir=params.model_dir,
...   model_fullyconv=True,
...   output_efieldx=128,
...   output_efieldy=128,
...   output_names="softmax_layer"
... )
2024-09-10 08:14:07 (INFO) [pyotb] TensorflowModelServe: argument for parameter "source1.il" was converted to list
INFO:pyotb:TensorflowModelServe: argument for parameter "source1.il" was converted to list
2024-09-10 08:14:07 (INFO) [pyotb] TensorflowModelServe: argument for parameter "output.names" was converted to list
INFO:pyotb:TensorflowModelServe: argument for parameter "output.names" was converted to list
2024-09-10 08:14:07.779852: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /data/models/model3
2024-09-10 08:14:07.787812: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }
2024-09-10 08:14:07.788059: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /data/models/model3
2024-09-10 08:14:07.799611: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:382] MLIR V1 optimization pass is not enabled
2024-09-10 08:14:07.804324: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.
2024-09-10 08:14:07.905226: I tensorflow/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.
2024-09-10 08:14:07.905262: I tensorflow/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.
2024-09-10 08:14:07.915719: I tensorflow/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.
2024-09-10 08:14:07.916613: W tensorflow/core/profiler/convert/xplane_to_step_stats.cc:78] GPU trace was not collected.
2024-09-10 08:14:07.917224: I tensorflow/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.
2024-09-10 08:14:07.918006: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /data/models/model3
2024-09-10 08:14:07.945099: I tensorflow/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.
2024-09-10 08:14:07.945126: I tensorflow/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.
2024-09-10 08:14:07.945199: I tensorflow/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.
2024-09-10 08:14:07.945221: W tensorflow/core/profiler/convert/xplane_to_step_stats.cc:78] GPU trace was not collected.
2024-09-10 08:14:07.945235: I tensorflow/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.
2024-09-10 08:14:07.945498: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 165860 microseconds.
2024-09-10 08:14:08 (INFO): Loading metadata from official product
2024-09-10 08:14:08 (INFO) TensorflowModelServe: Source info :
2024-09-10 08:14:08 (INFO) TensorflowModelServe: Receptive field  : [128, 128]
2024-09-10 08:14:08 (INFO) TensorflowModelServe: Placeholder name : input_ge
2024-09-10 08:14:08 (INFO) TensorflowModelServe: Output spacing ratio: 1
2024-09-10 08:14:08 (INFO) TensorflowModelServe: The TensorFlow model is used in fully convolutional mode
2024-09-10 08:14:08 (INFO) TensorflowModelServe: Setting background value to 0
2024-09-10 08:14:08 (INFO) TensorflowModelServe: Output field of expression: [128, 128]
2024-09-10 08:14:08 (WARNING) TensorflowModelServe: Aligning the tiling to the output expression field for better performances (dim 0). New value set to 128
2024-09-10 08:14:08 (WARNING) TensorflowModelServe: Aligning the tiling to the output expression field for better performances (dim 1). New value set to 128
2024-09-10 08:14:08 (INFO) TensorflowModelServe: Force tiling with squared tiles of [128, 128]
>>> 
>>> infer.write(
...   "/data/map_artifacts.tif",
...   ext_fname="box=2000:2000:1000:1000"
... )
2024-09-10 08:14:17 (INFO): Default RAM limit for OTB is 256 MB
2024-09-10 08:14:17 (INFO): GDAL maximum cache size is 392 MB
2024-09-10 08:14:17 (INFO): OTB will use at most 8 threads
2024-09-10 08:14:17 (INFO): Writing user defined region [2000, 2999]x[2000, 3000]
2024-09-10 08:14:17 (INFO): Estimated memory for full processing: 90.4678MB (avail.: 256 MB), optimal image partitioning: 1 blocks
2024-09-10 08:14:17 (INFO): File /data/map_artifacts.tif will be written in 1 blocks of 1000x1000 pixels
Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 0% [               Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 2% [*              Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 4% [**             Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 6% [***            Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 8% [****           Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 11% [*****         Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 12% [******        Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 14% [*******       Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 16% [********      Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 18% [*********     Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 20% [**********    Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 22% [***********   Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 24% [************  Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 27% [************* Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 28% [**************Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 30% [**************Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 32% [**************Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 34% [**************Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 37% [**************Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 38% [**************Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 40% [**************Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 43% [**************Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 44% [**************Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 46% [**************Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 48% [**************Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 50% [**************Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 53% [**************Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 54% [**************Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 56% [**************Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 58% [**************Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 60% [**************Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 62% [**************Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 64% [**************Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 66% [**************Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 69% [**************Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 70% [**************Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 72% [**************Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 74% [**************Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 76% [**************Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 79% [**************Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 80% [**************Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 82% [**************Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 85% [**************Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 86% [**************Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 88% [**************Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 90% [**************Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 92% [**************Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 95% [**************Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 96% [**************Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 98% [**************Writing /data/map_artifacts.tif?&box=2000:2000:1000:1000...: 100% [**************************************************] (0s)
True
>>> 
>>> 
>>> # inference without blocking artifacts
>>> 
>>> import pyotb
>>> import argparse
>>> 
>>> parser = argparse.ArgumentParser(description="Apply the model")
>>> parser.add_argument("--model_dir", required=True, help="model directory")
_StoreAction(option_strings=['--model_dir'], dest='model_dir', nargs=None, const=None, default=None, type=None, choices=None, required=True, help='model directory', metavar=None)
>>> params = parser.parse_args()
>>> 
>>> # Generate the classification map
>>> infer = pyotb.TensorflowModelServe(
...   n_sources=1,
...   source1_il="/data/ge.tif",
...   source1_rfieldx=128,
...   source1_rfieldy=128,
...   source1_placeholder="input_ge",
...   model_dir=params.model_dir,
...   model_fullyconv=True,
...   output_efieldx=64,
...   output_efieldy=64,
...   output_names="softmax_layer_crop32"
... )
2024-09-10 08:14:53 (INFO) [pyotb] TensorflowModelServe: argument for parameter "source1.il" was converted to list
INFO:pyotb:TensorflowModelServe: argument for parameter "source1.il" was converted to list
2024-09-10 08:14:53 (INFO) [pyotb] TensorflowModelServe: argument for parameter "output.names" was converted to list
INFO:pyotb:TensorflowModelServe: argument for parameter "output.names" was converted to list
2024-09-10 08:14:53.869900: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /data/models/model3
2024-09-10 08:14:53.883614: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }
2024-09-10 08:14:53.883746: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /data/models/model3
2024-09-10 08:14:53.897928: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.
2024-09-10 08:14:53.991216: I tensorflow/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.
2024-09-10 08:14:53.991257: I tensorflow/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.
2024-09-10 08:14:53.995310: I tensorflow/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.
2024-09-10 08:14:53.995351: W tensorflow/core/profiler/convert/xplane_to_step_stats.cc:78] GPU trace was not collected.
2024-09-10 08:14:53.995413: I tensorflow/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.
2024-09-10 08:14:53.995707: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /data/models/model3
2024-09-10 08:14:54.022032: I tensorflow/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.
2024-09-10 08:14:54.022058: I tensorflow/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.
2024-09-10 08:14:54.022163: I tensorflow/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.
2024-09-10 08:14:54.022193: W tensorflow/core/profiler/convert/xplane_to_step_stats.cc:78] GPU trace was not collected.
2024-09-10 08:14:54.022218: I tensorflow/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.
2024-09-10 08:14:54.022267: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 152422 microseconds.
Traceback (most recent call last):
  File "/home/otbuser/.local/lib/python3.10/site-packages/pyotb/core.py", line 799, in execute
    self.app.Execute()
  File "/opt/otbtf/lib/otb/python/otbApplication.py", line 2445, in Execute
    return _otbApplication.Application_Execute(self)
RuntimeError: Cannot open image /data/ge.tif. The file does not exist.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "<string>", line 4, in __init__
  File "/home/otbuser/.local/lib/python3.10/site-packages/pyotb/apps.py", line 67, in __init__
    super().__init__(name, *args, **kwargs)
  File "/home/otbuser/.local/lib/python3.10/site-packages/pyotb/core.py", line 613, in __init__
    self.execute()
  File "/home/otbuser/.local/lib/python3.10/site-packages/pyotb/core.py", line 801, in execute
    raise RuntimeError(
RuntimeError: TensorflowModelServe: error during during app execution (Cannot open image /data/ge.tif. The file does not exist.
>>> # Generate the classification map
>>> infer = pyotb.TensorflowModelServe(
...   n_sources=1,
...   source1_il="/data/GE_aoi1.tif",
...   source1_rfieldx=128,
...   source1_rfieldy=128,
...   source1_placeholder="input_ge",
...   model_dir=params.model_dir,
...   model_fullyconv=True,
...   output_efieldx=64,
...   output_efieldy=64,
...   output_names="softmax_layer_crop32"
... )
2024-09-10 08:15:04 (INFO) [pyotb] TensorflowModelServe: argument for parameter "source1.il" was converted to list
INFO:pyotb:TensorflowModelServe: argument for parameter "source1.il" was converted to list
2024-09-10 08:15:04 (INFO) [pyotb] TensorflowModelServe: argument for parameter "output.names" was converted to list
INFO:pyotb:TensorflowModelServe: argument for parameter "output.names" was converted to list
2024-09-10 08:15:04.049429: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /data/models/model3
2024-09-10 08:15:04.059908: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }
2024-09-10 08:15:04.060012: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /data/models/model3
2024-09-10 08:15:04.074259: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.
2024-09-10 08:15:04.169266: I tensorflow/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.
2024-09-10 08:15:04.169302: I tensorflow/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.
2024-09-10 08:15:04.173140: I tensorflow/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.
2024-09-10 08:15:04.173176: W tensorflow/core/profiler/convert/xplane_to_step_stats.cc:78] GPU trace was not collected.
2024-09-10 08:15:04.173239: I tensorflow/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.
2024-09-10 08:15:04.173504: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /data/models/model3
2024-09-10 08:15:04.200174: I tensorflow/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.
2024-09-10 08:15:04.200206: I tensorflow/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.
2024-09-10 08:15:04.200306: I tensorflow/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.
2024-09-10 08:15:04.200331: W tensorflow/core/profiler/convert/xplane_to_step_stats.cc:78] GPU trace was not collected.
2024-09-10 08:15:04.200347: I tensorflow/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.
2024-09-10 08:15:04.200381: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 150984 microseconds.
2024-09-10 08:15:04 (INFO): Loading metadata from official product
2024-09-10 08:15:04 (INFO) TensorflowModelServe: Source info :
2024-09-10 08:15:04 (INFO) TensorflowModelServe: Receptive field  : [128, 128]
2024-09-10 08:15:04 (INFO) TensorflowModelServe: Placeholder name : input_ge
2024-09-10 08:15:04 (INFO) TensorflowModelServe: Output spacing ratio: 1
2024-09-10 08:15:04 (INFO) TensorflowModelServe: The TensorFlow model is used in fully convolutional mode
2024-09-10 08:15:04 (INFO) TensorflowModelServe: Setting background value to 0
2024-09-10 08:15:04 (INFO) TensorflowModelServe: Output field of expression: [64, 64]
2024-09-10 08:15:04 (WARNING) TensorflowModelServe: Aligning the tiling to the output expression field for better performances (dim 0). New value set to 64
2024-09-10 08:15:04 (WARNING) TensorflowModelServe: Aligning the tiling to the output expression field for better performances (dim 1). New value set to 64
2024-09-10 08:15:04 (INFO) TensorflowModelServe: Force tiling with squared tiles of [64, 64]
>>> 
>>> infer.write(
...   "/data/map_valid.tif",
...   ext_fname="box=2000:2000:1000:1000"
... )
2024-09-10 08:15:08 (INFO): Writing user defined region [2000, 2999]x[2000, 3000]
2024-09-10 08:15:08 (INFO): Estimated memory for full processing: 160.78MB (avail.: 256 MB), optimal image partitioning: 1 blocks
2024-09-10 08:15:08 (INFO): File /data/map_valid.tif will be written in 1 blocks of 1000x1000 pixels
Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 0% [                   Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 2% [*                  Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 4% [**                 Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 6% [***                Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 8% [****               Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 10% [*****             Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 12% [******            Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 14% [*******           Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 16% [********          Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 18% [*********         Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 20% [**********        Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 22% [***********       Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 24% [************      Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 26% [*************     Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 28% [**************    Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 30% [***************   Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 32% [****************  Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 34% [***************** Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 36% [******************Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 38% [******************Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 40% [******************Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 42% [******************Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 44% [******************Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 46% [******************Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 48% [******************Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 50% [******************Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 52% [******************Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 54% [******************Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 56% [******************Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 58% [******************Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 60% [******************Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 62% [******************Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 64% [******************Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 66% [******************Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 68% [******************Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 70% [******************Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 72% [******************Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 74% [******************Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 76% [******************Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 78% [******************Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 80% [******************Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 82% [******************Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 84% [******************Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 86% [******************Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 88% [******************Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 90% [******************Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 92% [******************Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 94% [******************Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 96% [******************Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 98% [******************Writing /data/map_valid.tif?&box=2000:2000:1000:1000...: 100% [**************************************************] (1s)
True
>>> infer.write(
...   "/data/map_valid_all.tif"
... )
2024-09-10 08:15:40 (INFO): Estimated memory for full processing: 10410.8MB (avail.: 256 MB), optimal image partitioning: 41 blocks
2024-09-10 08:15:40 (INFO): File /data/map_valid_all.tif will be written in 49 blocks of 992x992 pixels
Writing /data/map_valid_all.tif...: 0% [                                        Writing /data/map_valid_all.tif...: 2% [*                                       Writing /data/map_valid_all.tif...: 4% [**                                      Writing /data/map_valid_all.tif...: 6% [***                                     Writing /data/map_valid_all.tif...: 8% [****                                    Writing /data/map_valid_all.tif...: 10% [*****                                  Writing /data/map_valid_all.tif...: 12% [******                                 Writing /data/map_valid_all.tif...: 14% [*******                                Writing /data/map_valid_all.tif...: 16% [********                               Writing /data/map_valid_all.tif...: 18% [*********                              Writing /data/map_valid_all.tif...: 20% [**********                             Writing /data/map_valid_all.tif...: 22% [***********                            Writing /data/map_valid_all.tif...: 24% [************                           Writing /data/map_valid_all.tif...: 26% [*************                          Writing /data/map_valid_all.tif...: 28% [**************                         Writing /data/map_valid_all.tif...: 30% [***************                        Writing /data/map_valid_all.tif...: 32% [****************                       Writing /data/map_valid_all.tif...: 34% [*****************                      Writing /data/map_valid_all.tif...: 36% [******************                     Writing /data/map_valid_all.tif...: 38% [*******************                    Writing /data/map_valid_all.tif...: 40% [********************                   Writing /data/map_valid_all.tif...: 42% [*********************                  Writing /data/map_valid_all.tif...: 44% [**********************                 Writing /data/map_valid_all.tif...: 46% [***********************                Writing /data/map_valid_all.tif...: 48% [************************               Writing /data/map_valid_all.tif...: 50% [*************************              Writing /data/map_valid_all.tif...: 52% [**************************             Writing /data/map_valid_all.tif...: 54% [***************************            Writing /data/map_valid_all.tif...: 56% [****************************           Writing /data/map_valid_all.tif...: 58% [*****************************          Writing /data/map_valid_all.tif...: 60% [******************************         Writing /data/map_valid_all.tif...: 62% [*******************************        Writing /data/map_valid_all.tif...: 64% [********************************       Writing /data/map_valid_all.tif...: 66% [*********************************      Writing /data/map_valid_all.tif...: 68% [**********************************     Writing /data/map_valid_all.tif...: 70% [***********************************    Writing /data/map_valid_all.tif...: 72% [************************************   Writing /data/map_valid_all.tif...: 74% [*************************************  Writing /data/map_valid_all.tif...: 76% [************************************** Writing /data/map_valid_all.tif...: 78% [***************************************Writing /data/map_valid_all.tif...: 80% [***************************************Writing /data/map_valid_all.tif...: 82% [***************************************Writing /data/map_valid_all.tif...: 84% [***************************************Writing /data/map_valid_all.tif...: 86% [***************************************Writing /data/map_valid_all.tif...: 88% [***************************************Writing /data/map_valid_all.tif...: 90% [***************************************Writing /data/map_valid_all.tif...: 92% [***************************************Writing /data/map_valid_all.tif...: 94% [***************************************Writing /data/map_valid_all.tif...: 96% [***************************************Writing /data/map_valid_all.tif...: 98% [***************************************Writing /data/map_valid_all.tif...: 100% [**************************************************] (47s)
True
>>> 
