Windows PowerShell
Copyright (C) Microsoft Corporation. All rights reserved.

Install the latest PowerShell for new features and improvements! https://aka.ms/PSWindows

PS C:\Users\Emilia> docker pull mdl4eo/otbtf:latest
latest: Pulling from mdl4eo/otbtf
Digest: sha256:e6a04e727e7c4731e87ad42bacd22ab94de7efc0338439b94f96cd02ad6a9cf4
Status: Image is up to date for mdl4eo/otbtf:latest
docker.io/mdl4eo/otbtf:latest

What's next:
    View a summary of image vulnerabilities and recommendations → docker scout quickview mdl4eo/otbtf:latest
PS C:\Users\Emilia> docker run -it --platform=linux/amd64 --mount type=bind,source="C:\Users\Emilia\Desktop\prova_semantic_dune",target=/data mdl4eo/otbtf:latest
To run a command as administrator (user "root"), use "sudo <command>".
See "man sudo_root" for details.

otbuser@5859e3efec35:~$ cd /data/
otbuser@5859e3efec35:/data$ pip install pyotb pystac_client planetary_computer
pip install argparse otbtf tensorflow
Defaulting to user installation because normal site-packages is not writeable
Collecting pyotb
  Downloading pyotb-2.0.2-py3-none-any.whl.metadata (3.3 kB)
Collecting pystac_client
  Downloading pystac_client-0.8.3-py3-none-any.whl.metadata (5.2 kB)
Collecting planetary_computer
  Downloading planetary_computer-1.0.0-py3-none-any.whl.metadata (7.4 kB)
Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.10/dist-packages (from pyotb) (1.26.4)
Requirement already satisfied: requests>=2.28.2 in /usr/local/lib/python3.10/dist-packages (from pystac_client) (2.31.0)
Collecting pystac>=1.10.0 (from pystac[validation]>=1.10.0->pystac_client)
  Downloading pystac-1.10.1-py3-none-any.whl.metadata (6.4 kB)
Collecting python-dateutil>=2.8.2 (from pystac_client)
  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)
Collecting click>=7.1 (from planetary_computer)
  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)
Collecting pydantic>=1.7.3 (from planetary_computer)
  Downloading pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149.4/149.4 kB 3.4 MB/s eta 0:00:00
Collecting pytz>=2020.5 (from planetary_computer)
  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)
Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from planetary_computer) (24.0)
Collecting python-dotenv (from planetary_computer)
  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)
Collecting annotated-types>=0.6.0 (from pydantic>=1.7.3->planetary_computer)
  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)
Collecting pydantic-core==2.23.4 (from pydantic>=1.7.3->planetary_computer)
  Downloading pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)
Requirement already satisfied: typing-extensions>=4.6.1 in /opt/otbtf/local/lib/python3.10/dist-packages (from pydantic>=1.7.3->planetary_computer) (4.11.0)
Collecting jsonschema~=4.18 (from pystac[validation]>=1.10.0->pystac_client)
  Downloading jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)
Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pystac_client) (1.16.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28.2->pystac_client) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28.2->pystac_client) (3.7)
Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28.2->pystac_client) (2.2.1)
Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28.2->pystac_client) (2024.2.2)
Collecting attrs>=22.2.0 (from jsonschema~=4.18->pystac[validation]>=1.10.0->pystac_client)
  Downloading attrs-24.2.0-py3-none-any.whl.metadata (11 kB)
Collecting jsonschema-specifications>=2023.03.6 (from jsonschema~=4.18->pystac[validation]>=1.10.0->pystac_client)
  Downloading jsonschema_specifications-2023.12.1-py3-none-any.whl.metadata (3.0 kB)
Collecting referencing>=0.28.4 (from jsonschema~=4.18->pystac[validation]>=1.10.0->pystac_client)
  Downloading referencing-0.35.1-py3-none-any.whl.metadata (2.8 kB)
Collecting rpds-py>=0.7.1 (from jsonschema~=4.18->pystac[validation]>=1.10.0->pystac_client)
  Downloading rpds_py-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)
Downloading pyotb-2.0.2-py3-none-any.whl (40 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.8/40.8 kB 2.5 MB/s eta 0:00:00
Downloading pystac_client-0.8.3-py3-none-any.whl (33 kB)
Downloading planetary_computer-1.0.0-py3-none-any.whl (14 kB)
Downloading click-8.1.7-py3-none-any.whl (97 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97.9/97.9 kB 1.8 MB/s eta 0:00:00
Downloading pydantic-2.9.2-py3-none-any.whl (434 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 434.9/434.9 kB 7.4 MB/s eta 0:00:00
Downloading pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 8.6 MB/s eta 0:00:00
Downloading pystac-1.10.1-py3-none-any.whl (182 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 182.9/182.9 kB 6.8 MB/s eta 0:00:00
Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 229.9/229.9 kB 7.3 MB/s eta 0:00:00
Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 508.0/508.0 kB 8.1 MB/s eta 0:00:00
Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)
Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)
Downloading jsonschema-4.23.0-py3-none-any.whl (88 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.5/88.5 kB 5.5 MB/s eta 0:00:00
Downloading attrs-24.2.0-py3-none-any.whl (63 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 63.0/63.0 kB 4.5 MB/s eta 0:00:00
Downloading jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)
Downloading referencing-0.35.1-py3-none-any.whl (26 kB)
Downloading rpds_py-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (354 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 354.8/354.8 kB 8.7 MB/s eta 0:00:00
Installing collected packages: pytz, rpds-py, python-dotenv, python-dateutil, pyotb, pydantic-core, click, attrs, annotated-types, referencing, pystac, pydantic, jsonschema-specifications, jsonschema, pystac_client, planetary_computer
Successfully installed annotated-types-0.7.0 attrs-24.2.0 click-8.1.7 jsonschema-4.23.0 jsonschema-specifications-2023.12.1 planetary_computer-1.0.0 pydantic-2.9.2 pydantic-core-2.23.4 pyotb-2.0.2 pystac-1.10.1 pystac_client-0.8.3 python-dateutil-2.9.0.post0 python-dotenv-1.0.1 pytz-2024.2 referencing-0.35.1 rpds-py-0.20.0

[notice] A new release of pip is available: 24.0 -> 24.2
[notice] To update, run: python3 -m pip install --upgrade pip
Defaulting to user installation because normal site-packages is not writeable
Collecting argparse
  Downloading argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)
Requirement already satisfied: otbtf in /src/otbtf (4.3.1)
Requirement already satisfied: tensorflow in /opt/otbtf/local/lib/python3.10/dist-packages (2.14.0)
Requirement already satisfied: absl-py>=1.0.0 in /opt/otbtf/local/lib/python3.10/dist-packages (from tensorflow) (2.1.0)
Requirement already satisfied: astunparse>=1.6.0 in /opt/otbtf/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)
Requirement already satisfied: flatbuffers>=23.5.26 in /opt/otbtf/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)
Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/otbtf/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)
Requirement already satisfied: google-pasta>=0.1.1 in /opt/otbtf/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)
Requirement already satisfied: h5py>=2.9.0 in /opt/otbtf/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)
Requirement already satisfied: libclang>=13.0.0 in /opt/otbtf/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)
Requirement already satisfied: ml-dtypes==0.2.0 in /opt/otbtf/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)
Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)
Requirement already satisfied: opt-einsum>=2.3.2 in /opt/otbtf/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)
Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)
Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.23.4)
Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow) (59.6.0)
Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)
Requirement already satisfied: termcolor>=1.1.0 in /opt/otbtf/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)
Requirement already satisfied: typing-extensions>=3.6.6 in /opt/otbtf/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)
Requirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/otbtf/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)
Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/otbtf/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)
Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/otbtf/local/lib/python3.10/dist-packages (from tensorflow) (1.62.2)
Requirement already satisfied: tensorboard<2.15,>=2.14 in /opt/otbtf/local/lib/python3.10/dist-packages (from tensorflow) (2.14.1)
Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /opt/otbtf/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)
Requirement already satisfied: keras<2.15,>=2.14.0 in /opt/otbtf/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)
Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)
Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/otbtf/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.29.0)
Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/otbtf/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)
Requirement already satisfied: markdown>=2.6.8 in /opt/otbtf/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.6)
Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.31.0)
Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/otbtf/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.2)
Requirement already satisfied: werkzeug>=1.0.1 in /opt/otbtf/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.0.2)
Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/otbtf/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.3)
Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/otbtf/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.4.0)
Requirement already satisfied: rsa<5,>=3.1.4 in /opt/otbtf/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)
Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/otbtf/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (2.0.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.7)
Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2.2.1)
Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2024.2.2)
Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/otbtf/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.5)
Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/otbtf/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.6.0)
Requirement already satisfied: oauthlib>=3.0.0 in /opt/otbtf/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)
Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)
Installing collected packages: argparse
Successfully installed argparse-1.4.0

[notice] A new release of pip is available: 24.0 -> 24.2
[notice] To update, run: python3 -m pip install --upgrade pip
otbuser@5859e3efec35:/data$ python
Python 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>>
>>> import pyotb
2024-09-25 07:37:47.273721: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-09-25 07:37:47 (INFO) [pyotb] Successfully loaded 117 OTB applications
>>> import pystac_client
>>> import planetary_computer
>>>
>>> import argparse
>>> import otbtf
2024-09-25 07:37:53.182363: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
>>> import tensorflow as tf
>>>
>>>
>>>
>>>
>>> # inference to observe blocking artifacts
>>>
>>> import sys
>>> import argparse
>>> import tensorflow as tf
>>>
>>> # Simulate the command-line arguments
>>> sys.argv = [
...     'part_3_train.py',  # Script name
...     '--model_dir', '/data/models/model3'
... ]
>>>
>>> import pyotb
>>>
>>> # Generate the classification map
>>> infer = pyotb.TensorflowModelServe(
...   n_sources=1,
...   source1_il="/data/GE_aoi1.tif",
...   source1_rfieldx=128,
...   source1_rfieldy=128,
...   source1_placeholder="input_ge",
...   model_dir=params.model_dir,
...   model_fullyconv=True,
...   output_efieldx=128,
...   output_efieldy=128,
...   output_names="softmax_layer"
... )
Traceback (most recent call last):
  File "<stdin>", line 7, in <module>
NameError: name 'params' is not defined
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>> import pyotb
>>> import pystac_client
>>> import planetary_computer
>>>
>>> import argparse
>>> import otbtf
>>> import tensorflow as tf
>>>
>>>
>>> # list files in the data folder
>>> import os
>>> os.getcwd()
'/data'
>>> os.listdir("/data/")
['ckpts', 'GE_aoi1.tif', 'GE_aoi1.tif.aux.xml', 'GE_aoi1_histogram.jpeg', 'GE_aoi1_sand.tif', 'logs', 'map_artifacts.tif', 'map_valid.tif', 'map_valid_all.tif', 'map_valid_all.tif.aux.xml', 'models', 'prova_semantic_dune.py', 'prova_semantic_dune.qgz', 'prova_semantic_dune.rtf', 'prova_semantic_dune_script', 'S2_aoi1.tif', 'test_ge_patches.tif', 'test_labels_patches.tif', 'train_ge_patches.tif', 'train_labels_patches.tif', 'tt.cpg', 'tt.dbf', 'tt.prj', 'tt.qmd', 'tt.shp', 'tt.shx', 'tt.tif', 'tt.tif.aux.xml', 'valid_ge_patches.tif', 'valid_labels_patches.tif', 'vec_remaining.geojson', 'vec_remaining.qmd', 'vec_test.geojson', 'vec_test.qmd', 'vec_train.geojson', 'vec_train.qmd', 'vec_valid.geojson', 'vec_valid.qmd']
>>> os.chdir("/data/")
>>>
>>>
>>> import pyotb
>>>
>>> vec_train = "/data/vec_train.geojson"
>>> vec_valid = "/data/vec_valid.geojson"
>>> vec_test = "/data/vec_test.geojson"
>>>
>>>
>>>
>>> # define some constants
>>> class_nb = 3             # number of classes
>>> inp_key_ge = "input_ge"    # model input ge
>>> tgt_key = "estimated"    # model target
>>>
>>> # helper to create otbtf dataset from lists of patches
>>> def create_otbtf_dataset(ge, labels):
...     return otbtf.DatasetFromPatchesImages(
...         filenames_dict={
...             "ge": ge,
...             "labels": labels
...         }
...     )
...
>>> # dataset preprocessing function
>>> def dataset_preprocessing_fn(sample):
...     return {
...         inp_key_ge: sample["ge"],
...         tgt_key: otbtf.ops.one_hot(labels=sample["labels"], nb_classes=class_nb)
...     }
...
>>> # TensorFlow dataset creation from lists of patches
>>> def create_dataset(ge, labels, batch_size=8):
...     otbtf_dataset = create_otbtf_dataset(ge, labels)
...     return otbtf_dataset.get_tf_dataset(
...         batch_size=batch_size,
...         preprocessing_fn=dataset_preprocessing_fn,
...         targets_keys=[tgt_key]
...     )
...
>>> # define convolution operator
>>> def conv(inp, depth, name, strides=2):
...     conv_op = tf.keras.layers.Conv2D(
...         filters=depth,
...         kernel_size=5,
...         strides=strides,
...         activation="relu",
...         padding="same",
...         name=name
...     )
...     return conv_op(inp)
...
>>> # define transposed convolution operator
>>> def tconv(inp, depth, name, activation="relu"):
...     tconv_op = tf.keras.layers.Conv2DTranspose(
...         filters=depth,
...         kernel_size=5,
...         strides=2,
...         activation=activation,
...         padding="same",
...         name=name
...     )
...     return tconv_op(inp)
...
>>> # build the model
>>> import otbtf
>>> class FCNNModel(otbtf.ModelBase):
...
...     def normalize_inputs(self, inputs):
...         return {
...             inp_key_ge: tf.cast(inputs[inp_key_ge], tf.float32) * 0.01,
...         }
...
...     def get_outputs(self, normalized_inputs):
...         norm_inp_ge = normalized_inputs[inp_key_ge]
...
...         cv1 = conv(norm_inp_ge, 16, "conv1")
...         cv2 = conv(cv1, 32, "conv2")
...         cv3 = conv(cv2, 64, "conv3")
...         cv4 = conv(cv3, 64, "conv4")
...         cv1t = tconv(cv4, 64, "conv1t") + cv3
...         cv2t = tconv(cv1t, 32, "conv2t") + cv2
...         cv3t = tconv(cv2t, 16, "conv3t") + cv1
...         cv4t = tconv(cv3t, class_nb, "softmax_layer", "softmax")
...
...         argmax_op = otbtf.layers.Argmax(name="argmax_layer")
...
...         return {tgt_key: cv4t, "estimated_labels": argmax_op(cv4t)}
...
>>>
>>>
>>> # custom metric for F1-Score (code from: https://stackoverflow.com/questions/64474463/custom-f1-score-metric-in-tensorflow)
>>>
>>> class FScore(tf.keras.metrics.Metric):
...
...     def __init__(self, class_id, name=None, **kwargs):
...         if not name:
...             name = f'f_score_{class_id}'
...         super().__init__(name=name, **kwargs)
...         self.f1 = self.add_weight(name='f1', initializer='zeros')
...         self.precision_fn = tf.keras.metrics.Precision(class_id=class_id)
...         self.recall_fn = tf.keras.metrics.Recall(class_id=class_id)
...
...     def update_state(self, y_true, y_pred, sample_weight=None):
...         p = self.precision_fn(y_true, y_pred)
...         r = self.recall_fn(y_true, y_pred)
...         # since f1 is a variable, we use assign
...         self.f1.assign(2 * ((p * r) / (p + r + 1e-6)))
...
...     def result(self):
...         return self.f1
...
...     def reset_state(self):
...         # we also need to reset the state of the precision and recall objects
...         self.precision_fn.reset_states()
...         self.recall_fn.reset_states()
...         self.f1.assign(0)
...
>>>
>>> # training setup
>>> def train(params, ds_train, ds_valid, ds_test):
...     strategy = tf.distribute.MirroredStrategy()
...     with strategy.scope():
...         model = FCNNModel(dataset_element_spec=ds_train.element_spec)
...
...         # Precision and recall for each class
...         metrics = [
...             cls(class_id=class_id)
...             for class_id in range(class_nb)
...             for cls in [tf.keras.metrics.Precision, tf.keras.metrics.Recall]
...         ]
...
...         # F1-Score for each class
...         metrics += [
...             FScore(class_id=class_id, name=f"fscore_cls{class_id}")
...             for class_id in range(class_nb)
...         ]
...
...         model.compile(
...             loss={tgt_key: tf.keras.losses.CategoricalCrossentropy()},
...             optimizer=tf.keras.optimizers.Adam(params.learning_rate),
...             metrics={tgt_key: metrics}
...         )
...         model.summary()
...         save_best_cb = tf.keras.callbacks.ModelCheckpoint(
...             params.model_dir,
...             mode="min",
...             save_best_only=True,
...             monitor="val_loss"
...         )
...         callbacks = [save_best_cb]
...         if params.log_dir:
...             callbacks.append(tf.keras.callbacks.TensorBoard(log_dir=params.log_dir))
...         if params.ckpt_dir:
...             ckpt_cb = tf.keras.callbacks.BackupAndRestore(backup_dir=params.ckpt_dir)
...             callbacks.append(ckpt_cb)
...
...         # Train the model
...         model.fit(
...             ds_train,
...             epochs=params.epochs,
...             validation_data=ds_valid,
...             callbacks=callbacks
...         )
...
...         # Final evaluation on the test dataset
...         model.load_weights(params.model_dir)
...         values = model.evaluate(ds_test, batch_size=params.batch_size)
...         for metric_name, value in zip(model.metrics_names, values):
...             print(f"{metric_name}: {100*value:.2f}")
...
>>>
>>>
>>> # build a simple parser to provide arguments
>>>
>>> import sys
>>> import argparse
>>> import tensorflow as tf
>>>
>>> # Simulate the command-line arguments
>>> sys.argv = [
...     'part_3_train.py',  # Script name
...     '--model_dir', '/data/models/model3',
...     '--log_dir', '/data/logs/model3',
...     '--epochs', '50',
...     '--ckpt_dir', '/data/ckpts/model3'
... ]
>>>
>>> # Now parse the arguments using argparse
>>> parser = argparse.ArgumentParser(description="Train a FCNN model")
>>> parser.add_argument("--model_dir", required=True, help="model directory")
_StoreAction(option_strings=['--model_dir'], dest='model_dir', nargs=None, const=None, default=None, type=None, choices=None, required=True, help='model directory', metavar=None)
>>> parser.add_argument("--log_dir", help="log directory")
_StoreAction(option_strings=['--log_dir'], dest='log_dir', nargs=None, const=None, default=None, type=None, choices=None, required=False, help='log directory', metavar=None)
>>> parser.add_argument("--batch_size", type=int, default=4)
_StoreAction(option_strings=['--batch_size'], dest='batch_size', nargs=None, const=None, default=4, type=<class 'int'>, choices=None, required=False, help=None, metavar=None)
>>> parser.add_argument("--learning_rate", type=float, default=0.0002)
_StoreAction(option_strings=['--learning_rate'], dest='learning_rate', nargs=None, const=None, default=0.0002, type=<class 'float'>, choices=None, required=False, help=None, metavar=None)
>>> parser.add_argument("--epochs", type=int, default=100)
_StoreAction(option_strings=['--epochs'], dest='epochs', nargs=None, const=None, default=100, type=<class 'int'>, choices=None, required=False, help=None, metavar=None)
>>> parser.add_argument("--ckpt_dir", help="Directory for checkpoints")
_StoreAction(option_strings=['--ckpt_dir'], dest='ckpt_dir', nargs=None, const=None, default=None, type=None, choices=None, required=False, help='Directory for checkpoints', metavar=None)
>>> params = parser.parse_args()
>>>
>>>
>>> # Print to verify
>>> print("Model directory:", params.model_dir)
Model directory: /data/models/model3
>>>
>>> tf.get_logger().setLevel('ERROR')
>>>
>>>
>>> # dataset intantiation
>>> ds_train = create_dataset(
...     ["/data/train_ge_patches.tif"],
...     ["/data/train_labels_patches.tif"],
... )
/opt/otbtf/lib/python3/dist-packages/osgeo/gdal.py:315: FutureWarning: Neither gdal.UseExceptions() nor gdal.DontUseExceptions() has been explicitly called. In GDAL 4.0, exceptions will be enabled by default.
  warnings.warn(
2024-09-25 07:39:02.705693: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
WARNING: AutoGraph could not transform <function dataset_preprocessing_fn at 0x7f62928ad630> and will run it as-is.
Cause: Unable to locate the source code of <function dataset_preprocessing_fn at 0x7f62928ad630>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
>>> ds_train = ds_train.shuffle(buffer_size=100)
>>>
>>> ds_valid = create_dataset(
...     ["/data/valid_ge_patches.tif"],
...     ["/data/valid_labels_patches.tif"],
... )
>>>
>>> ds_test = create_dataset(
...     ["/data/test_ge_patches.tif"],
...     ["/data/test_labels_patches.tif"],
... )
>>>
>>>
>>>
>>> # inference to observe blocking artifacts
>>>
>>> import sys
>>> import argparse
>>> import tensorflow as tf
>>>
>>> # Simulate the command-line arguments
>>> sys.argv = [
...     'part_3_train.py',  # Script name
...     '--model_dir', '/data/models/model3'
... ]
>>>
>>>
>>> import pyotb
>>>
>>> # Generate the classification map
>>> infer = pyotb.TensorflowModelServe(
...   n_sources=1,
...   source1_il="/data/GE_aoi1.tif",
...   source1_rfieldx=128,
...   source1_rfieldy=128,
...   source1_placeholder="input_ge",
...   model_dir=params.model_dir,
...   model_fullyconv=True,
...   output_efieldx=128,
...   output_efieldy=128,
...   output_names="softmax_layer"
... )
2024-09-25 07:39:13 (INFO) [pyotb] TensorflowModelServe: argument for parameter "source1.il" was converted to list
INFO:pyotb:TensorflowModelServe: argument for parameter "source1.il" was converted to list
2024-09-25 07:39:13 (INFO) [pyotb] TensorflowModelServe: argument for parameter "output.names" was converted to list
INFO:pyotb:TensorflowModelServe: argument for parameter "output.names" was converted to list
2024-09-25 07:39:13.755006: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /data/models/model3
2024-09-25 07:39:13.766542: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }
2024-09-25 07:39:13.766574: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /data/models/model3
2024-09-25 07:39:13.773865: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:382] MLIR V1 optimization pass is not enabled
2024-09-25 07:39:13.775515: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.
2024-09-25 07:39:13.862352: I tensorflow/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.
2024-09-25 07:39:13.862581: I tensorflow/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.
2024-09-25 07:39:13.898996: I tensorflow/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.
2024-09-25 07:39:13.899304: W tensorflow/core/profiler/convert/xplane_to_step_stats.cc:78] GPU trace was not collected.
2024-09-25 07:39:13.899393: I tensorflow/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.
2024-09-25 07:39:13.899802: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /data/models/model3
2024-09-25 07:39:13.919092: I tensorflow/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.
2024-09-25 07:39:13.919139: I tensorflow/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.
2024-09-25 07:39:13.919271: I tensorflow/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.
2024-09-25 07:39:13.919311: W tensorflow/core/profiler/convert/xplane_to_step_stats.cc:78] GPU trace was not collected.
2024-09-25 07:39:13.919337: I tensorflow/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.
2024-09-25 07:39:13.919367: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 164618 microseconds.
2024-09-25 07:39:13 (INFO): Loading metadata from official product
2024-09-25 07:39:13 (INFO) TensorflowModelServe: Source info :
2024-09-25 07:39:13 (INFO) TensorflowModelServe: Receptive field  : [128, 128]
2024-09-25 07:39:13 (INFO) TensorflowModelServe: Placeholder name : input_ge
2024-09-25 07:39:13 (INFO) TensorflowModelServe: Output spacing ratio: 1
2024-09-25 07:39:13 (INFO) TensorflowModelServe: The TensorFlow model is used in fully convolutional mode
2024-09-25 07:39:13 (INFO) TensorflowModelServe: Setting background value to 0
2024-09-25 07:39:13 (INFO) TensorflowModelServe: Output field of expression: [128, 128]
2024-09-25 07:39:13 (WARNING) TensorflowModelServe: Aligning the tiling to the output expression field for better performances (dim 0). New value set to 128
2024-09-25 07:39:13 (WARNING) TensorflowModelServe: Aligning the tiling to the output expression field for better performances (dim 1). New value set to 128
2024-09-25 07:39:13 (INFO) TensorflowModelServe: Force tiling with squared tiles of [128, 128]
>>>
>>> infer.write(
...   "/data/map_artifacts_kern5.tif",
...   ext_fname="box=2000:2000:1000:1000"
... )
2024-09-25 07:39:30 (INFO): Default RAM limit for OTB is 256 MB
2024-09-25 07:39:30 (INFO): GDAL maximum cache size is 791 MB
2024-09-25 07:39:30 (INFO): OTB will use at most 24 threads
2024-09-25 07:39:30 (INFO): Writing user defined region [2000, 2999]x[2000, 3000]
2024-09-25 07:39:30 (INFO): Estimated memory for full processing: 90.4678MB (avail.: 256 MB), optimal image partitioning: 1 blocks
2024-09-25 07:39:30 (INFO): File /data/map_artifacts_kern5.tif will be written in 1 blocks of 1000x1000 pixels
Writing /data/map_artifacts_kern5.tif?&box=2000:2000:1000:1000...: 100% [**************************************************] (1s)
True
>>>
>>> # inference without blocking artifacts
>>>
>>> import pyotb
>>> import argparse
>>>
>>> parser = argparse.ArgumentParser(description="Apply the model")
>>> parser.add_argument("--model_dir", required=True, help="model directory")
_StoreAction(option_strings=['--model_dir'], dest='model_dir', nargs=None, const=None, default=None, type=None, choices=None, required=True, help='model directory', metavar=None)
>>> params = parser.parse_args()
>>>
>>>
>>> # Generate the classification map
>>> infer = pyotb.TensorflowModelServe(
...   n_sources=1,
...   source1_il="/data/GE_aoi1.tif",
...   source1_rfieldx=128,
...   source1_rfieldy=128,
...   source1_placeholder="input_ge",
...   model_dir=params.model_dir,
...   model_fullyconv=True,
...   output_efieldx=64,
...   output_efieldy=64,
...   output_names="softmax_layer_crop32"
... )
2024-09-25 07:39:44 (INFO) [pyotb] TensorflowModelServe: argument for parameter "source1.il" was converted to list
INFO:pyotb:TensorflowModelServe: argument for parameter "source1.il" was converted to list
2024-09-25 07:39:44 (INFO) [pyotb] TensorflowModelServe: argument for parameter "output.names" was converted to list
INFO:pyotb:TensorflowModelServe: argument for parameter "output.names" was converted to list
2024-09-25 07:39:44.804223: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /data/models/model3
2024-09-25 07:39:44.809291: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }
2024-09-25 07:39:44.809317: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /data/models/model3
2024-09-25 07:39:44.815641: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.
2024-09-25 07:39:44.883832: I tensorflow/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.
2024-09-25 07:39:44.883879: I tensorflow/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.
2024-09-25 07:39:44.900018: I tensorflow/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.
2024-09-25 07:39:44.900050: W tensorflow/core/profiler/convert/xplane_to_step_stats.cc:78] GPU trace was not collected.
2024-09-25 07:39:44.900094: I tensorflow/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.
2024-09-25 07:39:44.900409: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /data/models/model3
2024-09-25 07:39:44.920239: I tensorflow/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.
2024-09-25 07:39:44.920286: I tensorflow/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.
2024-09-25 07:39:44.920417: I tensorflow/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.
2024-09-25 07:39:44.920441: W tensorflow/core/profiler/convert/xplane_to_step_stats.cc:78] GPU trace was not collected.
2024-09-25 07:39:44.920458: I tensorflow/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.
2024-09-25 07:39:44.920493: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 116276 microseconds.
2024-09-25 07:39:44 (INFO): Loading metadata from official product
2024-09-25 07:39:44 (INFO) TensorflowModelServe: Source info :
2024-09-25 07:39:44 (INFO) TensorflowModelServe: Receptive field  : [128, 128]
2024-09-25 07:39:44 (INFO) TensorflowModelServe: Placeholder name : input_ge
2024-09-25 07:39:44 (INFO) TensorflowModelServe: Output spacing ratio: 1
2024-09-25 07:39:44 (INFO) TensorflowModelServe: The TensorFlow model is used in fully convolutional mode
2024-09-25 07:39:44 (INFO) TensorflowModelServe: Setting background value to 0
2024-09-25 07:39:44 (INFO) TensorflowModelServe: Output field of expression: [64, 64]
2024-09-25 07:39:44 (WARNING) TensorflowModelServe: Aligning the tiling to the output expression field for better performances (dim 0). New value set to 64
2024-09-25 07:39:44 (WARNING) TensorflowModelServe: Aligning the tiling to the output expression field for better performances (dim 1). New value set to 64
2024-09-25 07:39:44 (INFO) TensorflowModelServe: Force tiling with squared tiles of [64, 64]
>>>
>>>
>>> infer.write(
...   "/data/map_valid_kern5.tif",
...   ext_fname="box=2000:2000:1000:1000"
... )
2024-09-25 07:39:53 (INFO): Writing user defined region [2000, 2999]x[2000, 3000]
2024-09-25 07:39:53 (INFO): Estimated memory for full processing: 160.78MB (avail.: 256 MB), optimal image partitioning: 1 blocks
2024-09-25 07:39:53 (INFO): File /data/map_valid_kern5.tif will be written in 1 blocks of 1000x1000 pixels
Writing /data/map_valid_kern5.tif?&box=2000:2000:1000:1000...: 100% [**************************************************] (1s)
True
>>>
>>> infer.write(
...   "/data/map_valid_all_kern5.tif"
... )
2024-09-25 07:40:01 (INFO): Estimated memory for full processing: 10410.8MB (avail.: 256 MB), optimal image partitioning: 41 blocks
2024-09-25 07:40:01 (INFO): File /data/map_valid_all_kern5.tif will be written in 49 blocks of 992x992 pixels
Writing /data/map_valid_all_kern5.tif...: 100% [**************************************************] (1m 27s)
True
>>>