{\rtf1\ansi\ansicpg1252\cocoartf2761
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fnil\fcharset0 HelveticaNeue;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red253\green147\blue106;\red254\green223\blue152;
\red0\green0\blue0;\red254\green223\blue152;\red0\green0\blue0;\red4\green41\blue57;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;\cssrgb\c100000\c64820\c48937;\cssrgb\c99898\c89464\c65948;
\csgray\c0\c0;\cssrgb\c99898\c89464\c65948;\csgray\c0;\cssrgb\c0\c21366\c28831;}
\paperw11900\paperh16840\margl1440\margr1440\vieww8600\viewh17260\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf2 \cb3 ### PROVA SEMANTIC SEGMENTATION su dune ################\cf0 \
\cb1 \
\cb4 # start Docker\cb5 \
\cb1 \
\cb6 # pull Docker image for OTBTF\cb1 \
docker pull mdl4eo/otbtf:latest\
\
\cb6 # mount data volumes (to access data from your local machine)\cb1 \
docker run -it --platform=linux/amd64 -v ~/Desktop/prova_semantic_dune:/data mdl4eo/otbtf:latest /bin/bash\
\
\cb6 # change directory\cb5 \
\cb1 cd /data/\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0 \cb6 # list files in the data folder within the directory\cb1 \
ls /data/\
\
\cb6 # install all required python packages\cb1 \
pip install pyotb pystac-client planetary-computer\
\
\cb6 # verify installation of packages\cb1 \
python\
\
import pyotb\
import pystac_client\
import planetary_computer\
\
import argparse\
import otbtf\
import tensorflow as tf\
\
\
\cb6 # list files in the data folder\cb1 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf7 \CocoaLigature0 import os\
\pard\pardeftab720\partightenfactor0
\cf0 \expnd0\expndtw0\kerning0
\CocoaLigature1 os.getcwd()\cf7 \kerning1\expnd0\expndtw0 \CocoaLigature0 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
os.listdir("/data/")\cf0 \CocoaLigature1 \
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0 \cb4 # patches selection in QGIS\cb5 \
\cb1 \
\cb6 # patches extraction\cb5 \
\cb1 \
import pyotb\
\
vec_train = "/data/vec_train.geojson"\
vec_valid = "/data/vec_valid.geojson"\
vec_test = "/data/vec_test.geojson"\
\
for vec in [vec_train, vec_valid, vec_test]:\
    \expnd0\expndtw0\kerning0
name = vec.split('/')[-1].replace("vec_", "").replace(".geojson", "")\kerning1\expnd0\expndtw0 \
    \
    app_extract = pyotb.PatchesExtraction(\
        n_sources=2,  \
        source1_il="/data/GE_aoi1.tif",\
        source1_patchsizex=64,\
        source1_patchsizey=64,\
        source1_nodata=0,\
        source2_il="/data/tt.tif",\
        source2_patchsizex=64,\
        source2_patchsizey=64,\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0         source2_nodata=0,\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0         vec=vec,\
        field="id"\
    )\
    \
    out_dict = \{\
        "source1.out": name + "_ge_patches.tif",\
        "source2.out": name + "_labels_patches.tif",\
    \}\
    \
    pixel_type = \{\
        "source1.out": "int16",\
        "source2.out": "uint8",\
    \}\
    \
    ext_fname = "gdal:co:COMPRESS=DEFLATE"\
    app_extract.write(out_dict, pixel_type=pixel_type, ext_fname=ext_fname)\
\
\
\cb6 # define some constants\cb5 \
\cb1 class_nb = 3             # number of classes\
inp_key_ge = "input_ge"    # model input ge\
tgt_key = "estimated"    # model target\
\
\cb6 # helper to create otbtf dataset from lists of patches\cb5 \
def create_otbtf_dataset(ge, labels):\
    return otbtf.DatasetFromPatchesImages(\
        filena\cb1 mes_dict=\{\
            "ge": ge,\
            "labels": labels\
        \}\
    )\
\
\cb6 # dataset preprocessing function\cb5 \
\cb1 def dataset_preprocessing_fn(sample):\
    return \{\
        inp_key_ge: sample["ge"],\
        tgt_key: otbtf.ops.one_hot(labels=sample["labels"], nb_classes=class_nb)\
    \}\
\
\cb6 # TensorFlow dataset creation from lists of patches\cb5 \
\cb1 def create_dataset(ge, labels, batch_size=8):\
    otbtf_dataset = create_otbtf_dataset(ge, labels)\
    return otbtf_dataset.get_tf_dataset(\
        batch_size=batch_size,\
        preprocessing_fn=dataset_preprocessing_fn,\
        targets_keys=[tgt_key]\
    )\
\
\cb6 # define convolution operator\cf8 \cb5 \
def conv(inp, depth, name, strides=2):\
    conv_op = tf.keras.layers.Conv2D(\
     \cf0 \cb1    filters=depth,\
        kernel_size=3,\
        strides=strides,\
        activation="relu",\
        padding="same",\
        name=name\
    )\
    return conv_op(inp)\
\
\cb6 # define transposed convolution operator\cb5 \
def\cb1  tconv(inp, depth, name, activation="relu"):\
    tconv_op = tf.keras.layers.Conv2DTranspose(\
        filters=depth,\
        kernel_size=3,\
        strides=2,\
        activation=activation,\
        padding="same",\
        name=name\
    )\
    return tconv_op(inp)\
\
\cb6 # build the model\cb5 \
import otbtf\cb1 \
class FCNNModel(otbtf.ModelBase):\
    \
    def normalize_inputs(self, inputs):\
        return \{\
            inp_key_ge: tf.cast(inputs[inp_key_ge], tf.float32) * 0.01,\
        \}\
    \
    def get_outputs(self, normalized_inputs):\
        norm_inp_ge = normalized_inputs[inp_key_ge]\
                \
        cv1 = conv(norm_inp_ge, 16, "conv1")\
        cv2 = conv(cv1, 32, "conv2")\
        cv3 = conv(cv2, 64, "conv3")\
        cv4 = conv(cv3, 64, "conv4")\
        cv1t = tconv(cv4, 64, "conv1t") + cv3\
        cv2t = tconv(cv1t, 32, "conv2t") + cv2\
        cv3t = tconv(cv2t, 16, "conv3t") + cv1\
        cv4t = tconv(cv3t, class_nb, "softmax_layer", "softmax")\
        \
        argmax_op = otbtf.layers.Argmax(name="argmax_layer")\
        \
        return \{tgt_key: cv4t, "estimated_labels": argmax_op(cv4t)\}\
\
\
\cb6 # custom metric for F1-Score (code from: https://stackoverflow.com/questions/64474463/custom-f1-score-metric-in-tensorflow)\cb1 \
\
class FScore(tf.keras.metrics.Metric):\
        \
    def __init__(self, class_id, name=None, **kwargs):\
        if not name:\
            name = f'f_score_\{class_id\}'\
        super().__init__(name=name, **kwargs)\
        self.f1 = self.add_weight(name='f1', initializer='zeros')\
        self.precision_fn = tf.keras.metrics.Precision(class_id=class_id)\
        self.recall_fn = tf.keras.metrics.Recall(class_id=class_id)\
    \
    def update_state(self, y_true, y_pred, sample_weight=None):\
        p = self.precision_fn(y_true, y_pred)\
        r = self.recall_fn(y_true, y_pred)\
        # since f1 is a variable, we use assign\
        self.f1.assign(2 * ((p * r) / (p + r + 1e-6)))\
    \
    def result(self):\
        return self.f1\
    \
    def reset_state(self):\
        # we also need to reset the state of the precision and recall objects\
        self.precision_fn.reset_states()\
        self.recall_fn.reset_states()\
        self.f1.assign(0)\
\
\
\cb6 # training setup\cb5 \
\pard\pardeftab720\partightenfactor0

\f1 \cf2 \cb5 \expnd0\expndtw0\kerning0
def train(params, ds_train, ds_valid, ds_test):\
    strategy = tf.distribute.MirroredStrategy()\
    with strategy.scope():\
        model = FCNNModel(dataset_element_spec=ds_train.element_spec)\
        \
        # Precision and recall for each class\
        metrics = [\
            cls(class_id=class_id)\
            for class_id in range(class_nb)\
            for cls in [tf.keras.metrics.Precision, tf.keras.metrics.Recall]\
        ]\
        \
        # F1-Score for each class\
        metrics += [\
            FScore(class_id=class_id, name=f"fscore_cls\{class_id\}")\
            for class_id in range(class_nb)\
        ]\
        \
        model.compile(\
            loss=\{tgt_key: tf.keras.losses.CategoricalCrossentropy()\},\
            optimizer=tf.keras.optimizers.Adam(params.learning_rate),\
            metrics=\{tgt_key: metrics\}\
        )\
        model.summary()\
        save_best_cb = tf.keras.callbacks.ModelCheckpoint(\
            params.model_dir,\
            mode="min",\
            save_best_only=True,\
            monitor="val_loss"\
        )\
        callbacks = [save_best_cb]\
        if params.log_dir:\
            callbacks.append(tf.keras.callbacks.TensorBoard(log_dir=params.log_dir))\
        if params.ckpt_dir:\
            ckpt_cb = tf.keras.callbacks.BackupAndRestore(backup_dir=params.ckpt_dir)\
            callbacks.append(ckpt_cb)\
        \
        # Train the model\
        model.fit(\
            ds_train,\
            epochs=params.epochs,\
            validation_data=ds_valid,\
            callbacks=callbacks\
        )\
        \
        # Final evaluation on the test dataset\
        model.load_weights(params.model_dir)\
        values = model.evaluate(ds_test, batch_size=params.batch_size)\
        for metric_name, value in zip(model.metrics_names, values):\
            print(f"\{metric_name\}: \{100*value:.2f\}")\
\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf2 \cb6 \kerning1\expnd0\expndtw0 # build a simple parser to provide arguments\cb5 \
\
import sys\
import argparse\
import tensorflow as tf\
\
\pard\pardeftab720\partightenfactor0
\cf2 \expnd0\expndtw0\kerning0
# Simulate the command-line arguments \
sys.argv = [ \
    'part_3_train.py',  # Script name \
    '--model_dir', '/data/models/model3', \
    '--log_dir', '/data/logs/model3',  \
    '--epochs', '50',  \
    '--ckpt_dir', '/data/ckpts/model3' \
]\
\
# Now parse the arguments using argparse\kerning1\expnd0\expndtw0 \
\expnd0\expndtw0\kerning0
parser = argparse.ArgumentParser(description="Train a FCNN model")\
parser.add_argument("--model_dir", required=True, help="model directory")\
parser.add_argument("--log_dir", help="log directory")\
parser.add_argument("--batch_size", type=int, default=4)\
parser.add_argument("--learning_rate", type=float, default=0.0002)\
parser.add_argument("--epochs", type=int, default=100)\
parser.add_argument("--ckpt_dir", help="Directory for checkpoints")\
params = parser.parse_args()\
\
# Print to verify \
print("Model directory:", params.model_dir)\
\
tf.get_logger().setLevel('ERROR')\kerning1\expnd0\expndtw0 \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf2 \
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0 \cf0 \cb6 # dataset intantiation\cb5 \
ds_train = create_dataset(\
    ["/data/train_ge_patches.tif"],\
    ["/data/train_labels_patches.tif"],\
)\
ds_train = ds_train.shuffle(buffer_size=100)\
\cb1 \
ds_valid = create_dataset(\
    ["/data/valid_ge_patches.tif"],\
    ["/data/valid_labels_patches.tif"],\
)\
\
ds_test = create_dataset(\
    ["/data/test_ge_patches.tif"],\
    ["/data/test_labels_patches.tif"],\
)\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f1 \cf2 \cb5 \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0 \cf0 \cb1 \
\cb6 # train the model\cb1 \
\pard\pardeftab720\partightenfactor0

\f1 \cf2 \cb5 \expnd0\expndtw0\kerning0
train(params, ds_train, ds_valid, ds_test)
\f0 \cf0 \cb1 \kerning1\expnd0\expndtw0 \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0 \
\
\cb6 # inference to observe blocking artifacts\cb5 \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f1 \cf2 \cb5 \
import sys\
import argparse\
import tensorflow as tf\
\
\pard\pardeftab720\partightenfactor0
\cf2 \expnd0\expndtw0\kerning0
# Simulate the command-line arguments \
sys.argv = [ \
    'part_3_train.py',  # Script name \
    '--model_dir', '/data/models/model3'\
]\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0 \cf0 \cb1 \kerning1\expnd0\expndtw0 \
import pyotb\
\
# Generate the classification map\
infer = pyotb.TensorflowModelServe(\
  n_sources=1,\
  source1_il="/data/GE_aoi1.tif",\
  source1_rfieldx=128,\
  source1_rfieldy=128,\
  source1_placeholder="input_ge",\
  model_dir=params.model_dir,\
  model_fullyconv=True,\
  output_efieldx=128,\
  output_efieldy=128,\
  output_names="softmax_layer"\
)\
\
infer.write(\
  "/data/map_artifacts.tif",\
  ext_fname="box=2000:2000:1000:1000"\
)\
\
\
\cb6 # inference without blocking artifacts\cb5 \
\
import pyotb\
import argparse\
\
parser = argparse.ArgumentParser(description="Apply the model")\
parser.add_argument("--model_dir", required=True, help="model directory")\
params = parser.parse_args()\cb1 \
\
# Generate the classification map\
infer = pyotb.TensorflowModelServe(\
  n_sources=1,\
  source1_il="/data/GE_aoi1.tif",\
  source1_rfieldx=128,\
  source1_rfieldy=128,\
  source1_placeholder="input_ge",\
  model_dir=params.model_dir,\
  model_fullyconv=True,\
  output_efieldx=64,\
  output_efieldy=64,\
  output_names="softmax_layer_crop32"\
)\
\
infer.write(\
  "/data/map_valid.tif",\
  ext_fname="box=2000:2000:1000:1000"\
)\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0 infer.write(\
  "/data/map_valid_all.tif"\
)\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0 \cb6 # create summary (to visualize in tensorboard)\cb5 \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f1 \cf2 \cb5 \
log_dir = "/data/logs/model3"\
\pard\pardeftab720\li320\fi-320\sa128\partightenfactor0
\cf2 \expnd0\expndtw0\kerning0
tf.summary.create_file_writer(log_dir)\
\
\cb6 # visualize in tensorboard (in a new terminal window)\cb5 \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0 \cf0 \cb5 \kerning1\expnd0\expndtw0 docker pull mdl4eo/otbtf:latest\
\cb1 \
docker run -it --platform=linux/amd64 -v ~/Desktop/prova_semantic_segmentation:/data -p 6006:6006 mdl4eo/otbtf:latest /bin/bash\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f1 \cf2 \cb5 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720\li320\fi-320\sa128\partightenfactor0
\cf2 pip install tensorboard\
tensorboard --logdir "/data/logs/model3"\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf2 \
\pard\pardeftab720\li320\fi-320\sa128\partightenfactor0
\cf2 \
\
\
\
\pard\pardeftab720\li320\fi-320\sa128\partightenfactor0

\f0 \cf0 \cb1 \kerning1\expnd0\expndtw0 \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0 \
}